{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain components to use\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
    "# you will also initialize the DB connection:\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTRA_DB_APPLICATION_TOKEN = \"\"\n",
    "ASTRA_DB_ID = \"41e053ea-c871-46d8-87d2-b769f70befe6\" #enter your Database ID\n",
    "GROQ_API_KEY = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader(r\"C:\\Users\\Uyama\\Downloads\\LANGCHAIN-PROJECTS\\RAG-Document\\research_papers\\LLM.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\\nNick Barnesh, Ajmal Miani\\naUniversity of Engineering and Technology (UET), Lahore, Pakistan\\nbThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ncUniversity of Technology Sydney (UTS), Sydney, Australia\\ndCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\neKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\nfSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\ngThe University of Melbourne (UoM), Melbourne, Australia\\nhAustralian National University (ANU), Canberra, Australia\\niThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\\non a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\\nconcepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\\nprovide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\\nextensive informative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans, and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over years containing keywords \"Large\\nLanguage Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\\nguage Model +Alignment\".\\nPreprint submitted to Elsevier April 11, 2024arXiv:2307.06435v9  [cs.CL]  9 Apr 20242019T5(Oct)\\nGPT-3(May)\\n WebGPT (Dec)\\nOPT -IML\\nTK-Instruct (May)\\nmT0 (Dec)\\n Wizard -LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard -Coder (Jun)\\nGoat\\nPanGu -α(Apr)\\nCPM -2(Jun)\\nGPT-NeoX -20B (Apr)\\nCodeGen (Mar)\\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2(Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020 2021 2022 2023 2024mT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic -1(Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0(Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN -U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\n PaLM2 (May)\\nGPT-4\\nPanGu -Σ(Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\\ntuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4], and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs. While\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot. Fine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc. These abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widelyadopted in diverse settings including, multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at a cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41]. Param-\\neter e fficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for e fficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions. Researchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n•We present a survey on the developments in LLM research\\nproviding a concise comprehensive overview of the direc-\\ntion.\\n•We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n•We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners e ffec-\\ntively leverage this technology.\\n•In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\n2Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. E fficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nhensively, including background, pre-training, fine-tuning,\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,architectures, training pipelines and strategies, fine-tuning, and\\nutilization in di fferent domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n32. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other. Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss di fferent attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs. To speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU (x)=max(0,x) (1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product ( ⊗) of a linear\\ntransformation and a sigmoid transformed ( σ) linear projection\\nof the input given as:\\nGLU (x,W,V,b,c)=(xW+b)⊗σ(xV+c), (2)\\nwhere Xis the input of layer and l,W,b,Vandcare learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU (x,W,V,b,c)=max(0,xW+b)⊗,\\nGEGLU (x,W,V,b,c)=GELU (xW+b)⊗(xV+c),\\nS wiGLU (x,W,V,b,c,β)=S wishβ(xW+b)⊗(xV+c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross di fferent devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n42.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can di fferenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write e fficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]: Provides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]: A deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C ++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based. Classifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can a ffect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The di fference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output. Here, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an e fficient\\nsparse architecture that o ffers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives. For\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting /utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of di fferent training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for di fferent\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with di fferent\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are di fferent styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries e ffectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting /Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing di fferent prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning. Generating\\nreasons is possible only by using di fferent prompting styles,whereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency: Improves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu- α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α[108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na=pnWq\\nhWk\\nhT HT\\nL (3)\\nCPM-2 [12]: Cost-e fficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-plete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-e fficient framework with a strat-\\negy to dynamically o ffload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model. The training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the e ffect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations , to its multi-task learning setup.\\n8It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx+Attn(LN1(x))+FF(LN2(x)) (4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with di fferences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90]. To gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7×larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3 ×GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little di fferent data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https: //github.com /TimDettmers /bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [ CLM ] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R,S,Xto rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter e fficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements e fficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nPanGu- Σ[92]: An autoregressive model with parameters\\ncopied from PanGu- αand extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated\\nand shared across all domains, whereas top layers are sparsely\\nactivated according to the domain. This training style allows\\nextracting task-specific models and reduces catastrophic forget-\\nting e ffects in the case of continual learning.\\n3.1.2. Coding\\nCodeGen [130]: CodeGen has similar architecture to\\nPaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\\nbeddings. The model is trained on both natural language and\\nprogramming language data sequentially (trained on the first\\ndataset, then the second and so on) on the following datasets\\n1) PILE, 2) BIGQUERY and 3) BIGPYTHON. CodeGen pro-\\nposed a multi-step approach to synthesizing code. The purpose\\nis to simplify the generation of long sequences where the previ-\\nous prompt and generated code are given as input with the next\\nprompt to generate the next code sequence. CodeGen open-\\nsource a Multi-Turn Programming Benchmark (MTPB) to eval-\\nuate multi-step program synthesis.Codex [131]: This LLM is trained on a subset of public Python\\nGithub repositories to generate code from docstrings. Com-\\nputer programming is an iterative process where the programs\\nare often debugged and updated before fulfilling the require-\\nments. Similarly to this, Codex generates 100 versions of a\\nprogram by repetitive sampling for a given description, which\\nproduces a working solution for 77.5% of the problems passing\\nunit tests. Its powerful version powers Github Copilot2.\\nAlphaCode [132]: A set of large language models, ranging\\nfrom 300M to 41B parameters, designed for competition-level\\ncode generation tasks. It uses the multi-query attention [133] to\\nreduce memory and cache costs. Since competitive program-\\nming problems highly require deep reasoning and an under-\\nstanding of complex natural language algorithms, the Alpha-\\nCode models are pre-trained on filtered GitHub code in popular\\nlanguages and then fine-tuned on a new competitive program-\\nming dataset named CodeContests. The CodeContests dataset\\nmainly contains problems, solutions, and test cases collected\\nfrom the Codeforces platform3. The pre-training employs stan-\\ndard language modeling objectives, while GOLD [134] with\\ntempering [135] serves as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\nCodeT5 +[34]: CodeT5 +is based on CodeT5 [136], with\\nshallow encoder and deep decoder, trained in multiple stages\\ninitially unimodal data (code) and later bimodal data (text-code\\npairs). Each training stage has di fferent training objectives and\\nactivates di fferent model blocks encoder, decoder, or both ac-\\ncording to the task. The unimodal pre-training includes span\\ndenoising and CLM objectives, whereas bimodal pre-training\\nobjectives contain contrastive learning, matching, and CLM for\\ntext-code pairs. CodeT5 +adds special tokens with the text to\\nenable task modes, for example, [ CLS ] for contrastive loss,\\n[Match ] for text-code matching, etc.\\nStarCoder [137]: A decoder-only model with the SantaCoder\\narchitecture, employing Flash attention to scale up the context\\nlength to 8k. The StarCoder trains an encoder to filter names,\\nemails, and other personal data from the training data. Its fine-\\ntuned variant outperforms PaLM, LLaMA, and LAMDA on\\nHumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge\\nGalactica [138]: A large curated corpus of human scientific\\nknowledge with 48 million papers, textbooks, lecture notes,\\nmillions of compounds and proteins, scientific websites, en-\\ncyclopedias, and more are trained using the metaseq library3,\\nwhich is built on PyTorch and fairscale [139]. The model wraps\\nreasoning datasets with the <work>token to provide step-by-\\nstep reasoning context to the model, which has been shown to\\nimprove the performance on reasoning tasks.\\n2https: //github.com /features /copilot\\n3https: //codeforces.com /\\n10Figure 10: This example illustrates the PanGu-Parchitecture, as depicted in\\nthe image sourced from [92].\\n3.1.4. Dialog\\nLaMDA [140]: A decoder-only model pre-trained on pub-\\nlic dialog data, public dialog utterances, and public web doc-\\numents, where more than 90% of the pre-training data is in\\nEnglish. LaMDA is trained with the objective of producing re-\\nsponses that exhibit high levels of quality, safety, and grounded-\\nness. To achieve this, discriminative and generative fine-tuning\\ntechniques are incorporated to enhance the model’s safety and\\nquality aspects. As a result, the LaMDA models can be utilized\\nas a general language model performing various tasks.\\n3.1.5. Finance\\nBloombergGPT [141]: A non-causal decoder model trained\\nusing both financial (\"FINPILE\" from the Bloomberg archive)\\nand general-purpose datasets. The model’s architecture is sim-\\nilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\\neters to di fferent blocks of the model using the approach [113].\\nFor e ffective training, BloombergGPT packs documents to-\\ngether with <|endo f text|>to use the maximum sequence\\nlength, uses warmup batch size starting from 1024 to 2048, and\\nmanually reduces the learning rate multiple times during the\\ntraining.\\nXuan Yuan 2.0 [142]: A Chinese financial chat model with\\nBLOOM’s [13] architecture trained on a combination of general\\npurpose, financial, general purpose instructions, and financial\\ninstitutions datasets. Xuan Yuan 2.0 combined the pre-training\\nand fine-tuning stages to avoid catastrophic forgetting.\\n3.2. Fine-Tuned LLMs\\nPre-trained LLMs have excellent generalization abilities to\\nunseen tasks. However, because they are generally trained with\\nthe objective of next token prediction, LLMs have limited ca-\\npacity to follow user intent and are prone to generate unethical,\\ntoxic or inaccurate responses [20]. For their e ffective utiliza-\\ntion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\\ngenerate safe responses [20], which also results in increasing\\nzero-shot, few-shot, and cross-task generalization [97, 16, 18],\\nFigure 11: An example image shows an instance of the Flan training paradigm,\\ntaken from [16].\\nwith minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].\\nWe review various fine-tuned LLMs and strategies for e ffective\\nfine-tuning in this section.\\n3.2.1. Instruction-Tuning with Manually Created Datasets\\nNumerous hand-crafted instruction-tuning datasets with\\ndifferent design choices are proposed in the literature to\\ninstruction-tune LLMs. The performance of fine-tuned LLMs\\ndepends on multiple factors, such as dataset, instruction diver-\\nsity, prompting templates, model size, and training objectives.\\nKeeping this in view, diverse fine-tuned models have emerged\\nin the literature using manually created datasets.\\nThe models T0 [17] and mT0 (multi-lingual) [144] employ\\ntemplates to convert existing datasets into prompt datasets.\\nThey have shown improvements in generalization to zero-shot\\nand held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\\nwith in-context instructions to study generalization on unseen\\ntasks when given in-context instructions during test time. The\\nmodel outperformed Instruct-GPT, despite being smaller in\\nsize, i.e., 11B parameters as compared to 175B of GPT-3.\\nIncreasing Tasks and Prompt Setups: Zero-shot and few-shot\\nperformance improves significantly by expanding task collec-\\ntion and prompt styles. OPT-IML [97] and Flan [16] curated\\nlarger 2k and 1.8k task datasets, respectively. While increasing\\ntask size alone is not enough, OPT-IML and Flan add more\\nprompting setups in their datasets, zero-shot, few-shot, and\\nCoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\\nfurther on 1.88M CoT samples. Another method [102] uses\\nsymbolic tasks with tasks in T0, Flan, etc.\\n3.2.2. Instruction-Tuning with LLMs Generated Datasets\\nGenerating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are often\\nwritten by humans, smaller in size, and less diverse. To\\novercome this, self-instruct [19] proposed an approach to\\nprompt available LLMs to generate instruction-tuning datasets.\\nSelf-instruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+tasks) [18] by 33%. It starts with a seed of 175 tasks,\\n1 instruction, and 1 sample per task and iteratively generates\\n11Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\\nModels Findings & Insights\\nT5•Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n•Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\\nclassification layers\\nGPT-3•Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\\nlearners\\nmT5•Large multi-lingual models perform equivalently to single language models on downstream tasks.\\nHowever, smaller multi-lingual models perform worse\\nPanGu-α •LLMs have good few shot capabilities\\nCPM-2•Prompt fine-tuning requires updating very few parameters while achieving performance compara-\\nble to full model fine-tuning\\n•Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n•Inserting prompt tokens in-between sentences can allow the model to understand relations between\\nsentences and long sequences\\n•In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\\n(aggregate information with the input text) for the model\\nERNIE 3.0•A modular LLM architecture with a universal representation module and task-specific representa-\\ntion module helps in the finetuning phase\\n•Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\\nan efficient way to take advantage of the powerful pre-trained model\\nJurassic-1•The performance of LLM is highly related to the network size\\n•To improve runtime performance, more operations can be performed in parallel (width) rather than\\nsequential (depth)\\n•To efficiently represent and fit more text in the same context length, the model uses a larger vo-\\ncabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\\nbenefits in few-shot learning tasks\\nHyperCLOV A•By employing prompt-based tuning, the performances of models can be improved, often surpassing\\nthose of state-of-the-art models when the backward gradients of inputs are accessible\\nYuan 1.0•The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\\nbehavior in zero-shot and few-shot learning\\nGopher •Relative encodings enable the model to evaluate for longer sequences than training.\\nERNIE 3.0 Titan•Additional self-supervised adversarial loss to distinguish between real and generated text improves\\nthe model performance as compared to ERNIE 3.0\\nGPT-NeoX-20B•Parallel attention +FF layers speed-up training 15% with the same performance as with cascaded\\nlayers\\n•Initializing feed-forward output layers before residuals with scheme in [143] avoids activations\\nfrom growing with increasing depth and width\\n•Training on Pile outperforms GPT-3 on five-shot\\nTable Continued on Next Page\\n12Models Findings & Insights\\nOPT•Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n•Model is prone to generate repetitive text and stuck in a loop\\nGalactica•Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\\ndomain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\\nresearch on LLMs\\n•A working memory token approach can achieve strong performance over existing methods on\\nmathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\\ntasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\nGLaM•The model capacity can be maintained at reduced computation by replacing the feed-forward layer\\nin each transformer layer with a mixture-of-experts (MoE)\\n•The model trained on filtered data shows consistently better performances on both NLG and NLU\\ntasks, where the e ffect of filtering is more significant on the former tasks\\n•Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\\nthe downstream tasks\\n•The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\\nthe MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\\nmance\\nLaMDA •The model can be fine-tuned to learn to call di fferent external information resources and tools\\nAlphaCode•For higher e ffectiveness and e fficiency, a transformer model can be asymmetrically constructed\\nwith a shallower encoder and a deeper decoder\\n•To achieve better performances, it is necessary to employ strategies such as massively scaling\\nupsampling, followed by the filtering and clustering of samples into a compact set\\n•The utilization of novel sampling-e fficient transformer architectures designed to facilitate large-\\nscale sampling is crucial\\n•Simplifying problem descriptions can e ffectively improve the model’s performance\\nChinchilla•The model size and the number of training tokens should be scaled proportionately: for each dou-\\nbling of the model size, the number of training tokens should be doubled as well\\nPaLM•English-centric models produce better translations when translating to English as compared to non-\\nEnglish\\n•Generalized models can have equivalent performance for language translation to specialized small\\nmodels\\n•Larger models have a higher percentage of training data memorization\\n•Performance has not yet saturated even at 540B scale, which means larger models are likely to\\nperform better\\nAlexaTM•Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\\ncontext than decoder-only\\n•Causal Language Modeling (CLM) task can be added to benefit the model with e fficient in-context\\nlearning\\n•Placing layer norm at the beginning of each transformer layer improves the training stability\\nTable Continued on Next Page\\n13Models Findings & Insights\\nU-PaLM•Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n•Training with a mixture of denoisers improves the infilling ability and open-ended text generation\\ndiversity\\nUL2•Mode switching training enables better performance on downstream tasks\\n•CoT prompting outperforms standard prompting for UL2\\nGLM-130B•Pre-training data with a small proportion of multi-task instruction data improves the overall model\\nperformance\\nCodeGen•Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\\neration\\nLLaMA•A constant performance improvement is observed when scaling the model\\n•Smaller models can achieve good performances with more training data and computing time\\nPanGu- Σ•Sparse models provide the benefits of large models at a lower computation cost\\n•Randomly Routed Experts reduces catastrophic forgetting e ffects which in turn is essential for\\ncontinual learning\\n•Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\\ncost-e fficient while maintaining a performance similar to the original\\nBloombergGPT•Pre-training with general-purpose and task-specific data improves task performance without hurt-\\ning other model capabilities\\nXuanYuan 2.0 •Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5 +•Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n•Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\\nfor better performance\\nStarCoder •HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2•Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\\nfine-tuning\\n•Model trained on unfiltered data requires fewer samples for safety alignment\\nPaLM-2•Data quality is important to train better models\\n•Model and data size should be scaled with 1:1 proportions\\n•Smaller models trained for larger iterations outperform larger models\\n14Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels Findings & Insights\\nT0•Multi-task prompting enables zero-shot generalization and outperforms baselines\\n•Even a single prompt per dataset task is enough to improve performance\\nWebGPT•To aid the model in e ffectively filtering and utilizing relevant information, human labelers play a\\ncrucial role in answering questions regarding the usefulness of the retrieved documents\\n•Interacting a fine-tuned language model with a text-based web-browsing environment can improve\\nend-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n•Generating answers with references can make labelers easily judge the factual accuracy of answers\\nTk-INSTRUCT•Instruction tuning leads to a stronger generalization of unseen tasks\\n•More tasks improve generalization whereas only increasing task instances does not help\\n•Supervised trained models are better than generalized models\\n•Models pre-trained with instructions and examples perform well for di fferent types of inputs\\nmT0 and BLOOMZ•Instruction tuning enables zero-shot generalization to tasks never seen before\\n•Multi-lingual training leads to even better zero-shot generalization for both English and non-\\nEnglish\\n•Training on machine-translated prompts improves performance for held-out tasks with non-English\\nprompts\\n•English only fine-tuning on multilingual pre-trained language model is enough to generalize to\\nother pre-trained language tasks\\nOPT-IML•Creating a batch with multiple task examples is important for better performance\\n•Only example proportional sampling is not enough, training datasets should also be proportional\\nfor better generalization /performance\\n•Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\\nwhereas fully supervised tasks have no e ffect\\n•Including small amounts i.e. 5% of pretraining data during fine-tuning is e ffective\\n•Only 1% reasoning data improves the performance, adding more deteriorates performance\\n•Adding dialogue data makes the performance worse\\nSparrow•Labelers’ judgment and well-defined alignment rules help the model generate better responses\\n•Good dialogue goals can be broken down into detailed natural language rules for the agent and the\\nraters\\n•The combination of reinforcement learning (RL) with reranking yields optimal performance in\\nterms of preference win rates and resilience against adversarial probing\\nFlan•Finetuning with CoT improves performance on held-out tasks\\n•Fine-tuning along with CoT data improves reasoning abilities\\n•CoT tuning improves zero-shot reasoning\\n•Performance improves with more tasks\\n•Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n•Improving the model’s performance with instruction tuning is compute-e fficient\\n•Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder •Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\nLLaMA-2-Chat•Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\\nRLHF step further improves model safety and make it less prone to jailbreak attacks\\nLIMA •Less high quality data is enough for fine-tuned model generalization\\n15new instructions (52k) and instances (82k input-output pairs)\\nusing GPT-3 [6]. Contrary to this, Dynosaur [145] uses the\\nmeta-data of datasets on Huggingface to prompt LLMs to\\ngenerate multiple task instruction-tuning datasets.\\nLLaMA Tuned: Various models in the literature instruction-\\ntune LLaMA [146] with GPT-3 [6] or GPT-4 [147] generated\\ndatasets. Among these, Alpaca [148], Vicuna [149], and\\nLLaMA-GPT-4 [150] are a few general-purpose fine-tuned\\nmodels, where Alpaca is trained on 52k samples from text-\\ndavinci-003, Vicuna on 70k samples from ShareGPT.com,\\nand LLaMA-GPT-4 by re-creating Alpaca instructions from\\nGPT-4. Goat [151] fine-tunes LLaMA for arithmetic tasks\\n(1 million samples) by generating data from ChatGPT and\\noutperforms GPT-4, PaLM, BLOOM, OPT, etc., attributing its\\nsuccess to the LLaMA’s consistent tokenization of numbers.\\nHuaTuo [152] is a medical knowledge model, fine-tuned with\\na generated QA dataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [153, 154] prompts\\nLLMs to convert given instructions into a more complex set.\\nThe instructions are iteratively evolved with re-writing instruc-\\ntions in complex wording and creating new instructions. With\\nthis style of automated instruction generation, WizardLM [153]\\n(fine-tuned LLaMA on 250k instructions), outperforms Vicuna\\nand Alpaca, and WizardCoder [154] (fine-tuned StarCoder)\\nbeats Claude-Plus, Bard, and others.\\n3.2.3. Aligning with Human Preferences\\nIncorporating human preferences into LLMs presents a\\nsignificant advantage in mitigating undesirable behaviors and\\nensuring accurate outputs. The initial work on alignment, such\\nas InstructGPT [20] aligns GPT-3 using a 3-step approach,\\ninstruction-tuning, reward modeling, and fine-tuning with\\nreinforcement learning (RL). The supervised fine-tuned GPT-3\\non demonstrations is queried to generate responses, which\\nhuman labelers rank according to human values, and a reward\\nmodel is trained on the ranked data. Lastly, the GPT-3 is trained\\nwith proximal policy optimization (PPO) using rewards on the\\ngenerated data from the reward model. LLaMA 2-Chat [21]\\nimproves alignment by dividing reward modeling into help-\\nfulness and safety rewards and using rejection sampling in\\naddition to PPO. The initial four versions of LLaMA 2-Chat\\nare fine-tuned with rejection sampling and then with PPO on\\ntop of rejection sampling.\\nAligning with Supported Evidence: This style of alignment\\nallows the model to generate responses with proofs and facts,\\nreduces hallucination, and assists humans more e ffectively,\\nwhich increases trust in the model’s output. Similar to\\nthe RLHF training style, a reward model is trained to rank\\ngenerated responses containing web citations in answers\\nto questions, which is later used to train the model, as in\\nGopherCite [155], WebGPT [156], and Sparrow [157]. The\\nranking model in Sparrow [157] is divided into two branches,\\npreference reward and rule reward, where human annotators\\nadversarial probe the model to break a rule. These two rewards\\ntogether rank a response to train with RL.Aligning Directly with SFT: The PPO in the RLHF pipeline\\nis complex, memory-intensive, and unstable, requiring mul-\\ntiple models, reward, value, policy, and reference models.\\nAvoiding this sophisticated alignment pipeline is possible by\\nincorporating minimal changes in the supervised fine-tuning\\n(SFT) pipeline as in [158, 159, 160], with better or compa-\\nrable performance to PPO. Direct preference optimization\\n(DPO) [158] trains a model directly on the human-preferred\\nresponses to maximize the likelihood of preferred against\\nunpreferred responses, with per-sample importance weight.\\nReward ranked fine-tuning RAFT [159] fine-tunes the model\\non ranked responses by the reward model. Preference ranking\\noptimization (PRO) [161] and RRHF [160] penalize the model\\nto rank responses with human preferences and supervised loss.\\nOn the other hand, chain-of-hindsight (CoH) [162] provides\\nfeedback to the model in language rather than reward, to learn\\ngood versus bad responses.\\nAligning with Synthetic Feedback: Aligning LLMs with\\nhuman feedback is slow and costly. The literature suggests a\\nsemi-automated process to align LLMs by prompting LLMs to\\ngenerate helpful, honest, and ethical responses to the queries,\\nand fine-tuning using the newly created dataset. Constitutional\\nAI [163] replaces human feedback in RLHF with AI, calling\\nit RL from AI feedback (RLAIF). AlpacaFarm [164] designs\\nprompts to imitate human feedback using LLMs APIs. Oppo-\\nsite to constitutional AI, AlpacaFarm injects noise in feedback\\nto replicate human mistakes. Self-Align [98] prompts the\\nLLM with ICL examples, instructing the LLM about what the\\nresponse should contain to be considered useful and ethical.\\nThe same LLM is later fine-tuned with the new dataset.\\nAligning with Prompts: LLMs can be steered with prompts to\\ngenerate desirable responses without training [165, 166]. The\\nself-correction prompting in [166] concatenates instructions\\nand CoT with questions, guiding the model to answer its\\ninstruction following a strategy to ensure moral safety before\\nthe actual answer. This strategy is shown to reduce the harm in\\ngenerated responses significantly.\\nRed-Teaming /Jailbreaking /Adversarial Attacks: LLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-\\nformation, and other shortcomings through adversarial probing.\\nThe models are susceptible to generating harmful responses\\neven though they are aligned for safety [167, 168]. Red-\\nteaming is a common approach to address illicit outputs, where\\nthe LLMs are prompted to generate harmful outputs [168, 169].\\nThe dataset collected through red-teaming is used to fine-tune\\nmodels for safety. While red-teaming largely relies on human\\nannotators, another work [170] red-team LLMs to find prompts\\nthat lead to harmful outputs for other LLMs.\\n3.2.4. Continue Pre-Training\\nAlthough fine-tuning boosts a model’s performance, it leads\\nto catastrophic forgetting of previously learned information.\\nConcatenating fine-tuning data with a few randomly selected\\npre-training samples in every iteration avoids network forget-\\nting [171, 142]. This is also e ffective in adapting LLMs for\\ncases where fine-tuning data is small and the original capac-\\n16ity is to be maintained. Prompt-based continued pre-training\\n(PCP) [172] trains the model with text and instructions related\\nto tasks and then finally instruction-tunes the model for down-\\nstream tasks.\\n3.2.5. Sample E fficiency\\nWhile fine-tuning data is generally many-fold smaller than\\nthe pre-training data, it still has to be large enough for accept-\\nable performance [16, 97, 18] and requires proportional com-\\nputing resources. Studying the e ffects on performance with less\\ndata, existing literature [173, 174] finds that models trained\\non less data can outperform models trained with more data.\\nIn [173], 25% of the total downstream data is found enough\\nfor state-of-the-art performance. Selecting coreset-based 0.5%\\nof the total instruction-tuning data improves the model perfor-\\nmance by 2% in [174], as compared to the complete data tun-\\ning. Less is more for alignment (LIMA) [175] uses only 1000\\ncarefully created demonstrations to fine-tune the model and has\\nachieved comparable performance to GPT-4.\\n3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-\\npensive attention and high memory requirements. A model\\ntrained on limited sequence lengths fails to generalize to unseen\\nlengths at inference time [176, 49]. Alternatively, LLMs with\\nALiBi [65] positional encodings can perform zero-shot length\\nextrapolation. However, ALiBi has less expressive power [66]\\nand inferior performance on multiple benchmarks [46], and\\nmany LLMs use RoPE positional embedding that is unable to\\nperform zero-shot extrapolation. A larger context length has\\nbenefits such as a better understanding of longer documents,\\nmore samples in in-context learning, execution of bigger rea-\\nsoning processes, etc. Expanding context length during fine-\\ntuning is slow, ine fficient, and computationally expensive [49].\\nTherefore, researchers employ various context window extrap-\\nolation techniques discussed below.\\nPosition Interpolation: Rather than extrapolating, [49] shows\\nthat interpolating position encodings within the pre-trained con-\\ntext window are more e ffective. The work demonstrates that\\nonly 1000 steps of fine-tuning are enough to achieve better re-\\nsults on larger windows without reducing performance com-\\npared to the original context size. Gira ffe [46] uses power scal-\\ning in RoPE, and YaRN [47] proposed NTK-aware interpola-\\ntion.\\nEfficient Attention Mechanism: Dense global attention is\\none of the major constraints in training larger context win-\\ndow LLMs. Using e fficient attention variants, such as lo-\\ncal, sparse, and dilated attention, reduces the computation cost\\nsignificantly. LongT5 [48] proposes transient global atten-\\ntion (TGlobal), applying attention to local and global tokens\\n(windowed token averaging). The model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on\\n4098 sequence length, fine-tunes on larger window sizes, as\\nlarge as 16k, and improves task performance on longer inputs.\\nThis shows the extrapolation ability of TGlobal attention with\\nonly fine-tuning. COLT5 [177] uses two branches, one withlightweight and the other with heavyweight attention and feed-\\nforward layers. All tokens are processed from the lightweight\\nbranch, and only important tokens are routed to the heavy-\\nweight branch. LongNet [178] replaces standard attention with\\ndilated attention, expanding sequence length to 1 billion tokens.\\nLongLoRA [179] proposes shift-short attention, used during\\nfine-tuning to reduce dense attention costs. However, the model\\nduring inference uses dense attention and achieves similar per-\\nformance as full attention fine-tuning.\\nExtrapolation without Training: LM-Infinite [176] and par-\\nallel context windows (PCW) [180] show length extrapolation\\nis possible using pre-trained LLMs. LM-Infinite suggested Λ-\\nshaped attention applied within the original context window\\nlimits. Likewise, PCW chunks larger inputs into the pre-trained\\ncontext lengths and applies the same positional encodings to\\neach chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-\\nnated with the input, known as context augmentation, in-\\ncontext learning (ICL), or few-shot prompting. They show ex-\\ncellent generalization to unseen tasks with few-shot prompt-\\ning, enabling LLMs to answer queries beyond the capacity ac-\\nquired during training [6, 55]. These emergent abilities allow\\nfor adapting the model without fine-tuning—a costly process.\\nAside from this, hallucination, producing inaccurate, unsafe,\\nor factually incorrect responses, is common for LLMs, which is\\navoided by augmenting contextual data. While the user can pro-\\nvide in-context samples in the query [54, 32], here we specifi-\\ncally refer to the methods that access external storage program-\\nmatically, calling them augmented LLMs.\\nThe literature suggests various external memory designs to aug-\\nment LLMs, long-term [181, 182, 183, 184], short-term [185],\\nsymbolic [186], and non-symbolic [187, 188]. The memory\\ncan be maintained in di fferent formats such as documents, vec-\\ntors, or databases. A few systems maintain intermediate mem-\\nory representations to retain information across multiple iter-\\nations [184, 182], while others extract important information\\nfrom the datasets and save it in memory for recall [189]. The\\nmemory read and write operations are performed either with\\nor without LLMs cooperation [182, 190, 184, 191], acting as\\na feedback signal in [185]. We discuss di fferent types of aug-\\nmented LLMs below.\\n3.4.1. Retrieval Augmented LLMs\\nLLMs may have limited memory and outdated information,\\nleading to inaccurate responses. Retrieving relevant informa-\\ntion from external up-to-date storage enables the LLMs to\\naccurately answer with references and utilize more informa-\\ntion. With retrieval augmentation, smaller models have been\\nshown to perform at par with larger models. For instance, the\\n11B model can become competitive to 540B PaLM in [25] and\\n7.5B to 280B Gopher in [183]. Retrieval augmented language\\nmodeling (RALM) has two major components, shown in\\nFigure 12, namely: 1) retriever and 2) language model. In\\nRALM, the retriever plays a crucial role in driving LLM\\n17Figure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\\ntracts a similar context to the input and forwards it to the LLM either in simple\\nlanguage or encoded through Fusion-in-Decoder (FiD). Depending on the task,\\nretrieval and generation may repeat multiple times.\\nresponse, where incorrect information can steer LLMs to false\\nbehavior. This leads to the development of various methods to\\nretrieve accurate information and fuse with the query for better\\nperformance.\\nZero-Shot Retrieval Augmentation: This kind of augmen-\\ntation keeps the original LLM architecture and weights\\nunchanged and uses BM25 [192], nearest neighbors, or frozen\\npre-trained models like Bert [7] as a retriever. The retrieved\\ninformation is provided as input to the model for response\\ngeneration, shown to improve performance over LLMs without\\nretrieval [188, 193]. In some scenarios, multiple retrieval\\niterations are required to complete the task. The output\\ngenerated in the first iteration is forwarded to the retriever\\nto fetch similar documents. Forward-looking active retrieval\\n(FLARE) [187] initially generates the response and corrects\\nthe output by retrieving relevant documents if the response\\ncontains low-confidence tokens. Similarly, RepoCoder [194]\\nfetches code snippets recursively for code completion.\\nTraining with Retrieval Augmentation: To reduce failures in\\nretrieval augmentation generation (RAG), researchers train or\\nfine-tune retrievers and LLMs with a retrieval augmentation\\npipeline. We discuss the literature below based on their focus\\non the respective training processes of the pipeline.\\nTraining LLM: Retrieval-enhanced transformer (RETRO) [183]\\nshows pre-training smaller LLMs with RAG pipeline outper-\\nforms larger LLMs, such as GPT-3 trained without RAG.\\nRETRO uses a 2-trillion token subset of MassiveText as\\na database. The retrieval pipeline divides the input query\\ninto subsets and retrieves relevant chunks from the database\\nfor each subset, encoded together with input intermediate\\nrepresentations for generating tokens. It uses cross-chunked\\nattention to attend to previous chunks auto-regressively. A\\nstudy on RETRO [195] shows models pre-trained without RAG\\nbut fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.\\nTraining Retriever: Quality of responses generated by LLMs\\nis highly dependent on the in-context examples. There-\\nfore, [196, 197, 198, 199] train retrievers to retrieve accurate\\nfew-shot samples while keeping the LLM frozen for gener-ation. Retrieved samples are ranked to build ground-truth\\ndata to train retrievers with contrastive learning in [196, 198].\\nRoBERTa is trained for downstream tasks in [197] for ICL\\nsamples retrieval. REPLUG [199] trains the retriever with\\nsupervised signals from the frozen LLM-generated outputs.\\nTraining Retriever and LLM: Further benefits are achieved by\\ntraining both the retriever and the model in [25, 200, 201]. In\\nthis case, the error propagates back to the retriever, updating\\nboth the language model and the retriever. While masked\\nlanguage modeling (MLM) is a common pre-training objec-\\ntive [25, 201], retrieval pre-trained transformer (RPT) [200]\\nused document chunk prediction as a pre-training objective for\\nlong text modeling.\\nEncoded Context Augmentation: Concatenating retrieved\\ndocuments with the query becomes infeasible as the sequence\\nlength and sample size grow. Encoding the context and fusing\\nit with the decoder (Fusion-in-Decoder) using cross-attention\\nmakes it possible to augment more samples without increasing\\ncomputation costs significantly [202, 183, 200, 25].\\nWeb Augmented: Locally stored memory, but external to\\nLLM, has limited information. However, a large amount of\\ninformation is available on the internet, which gets updated\\nregularly. Rather than storing information locally, various\\nmethods retrieve query-related context through a web search\\nand forward it to LLMs [203, 204, 156].\\n3.4.2. Tool Augmented LLMs\\nWhile RAG relies on the retriever to provide context to the\\nLLM to answer queries, tool augmented LLMs capitalize on the\\nreasoning abilities of LLMs to iteratively plan by dividing tasks\\ninto sub-tasks, selecting necessary tools, and taking actions to\\ncomplete the task [205, 206, 207, 27]. A generic pipeline of\\ntool-augmented LLMs is shown in Figure 13, where di fferent\\nmodules in Figure 13 are selected in a loop until the task com-\\npletion.\\nZero-Shot Tool Augmentation: LLMs in-context learning\\nand reasoning abilities enable them to interact with tools with-\\nout training. Automatic reasoning and tool-use (ART) [207]\\nbuilds a task library with demonstrations of reasoning steps and\\ncalling external tools. It retrieves similar task examples and\\nprovides the context to the LLM for inference. Aside from\\nthis, [208] shows tool documentation is enough to teach LLMs\\nto use tools without demonstrations. RestGPT [209] integrates\\nLLMs with RESTful APIs by decomposing tasks into planning\\nand API selection steps. The API selector understands the API\\ndocumentation to select a suitable API for the task and plan the\\nexecution. ToolkenGPT [210] uses tools as tokens by concate-\\nnating tool embeddings with other token embeddings. During\\ninference, the LLM generates the tool tokens representing the\\ntool call, stops text generation, and restarts using the tool exe-\\ncution output.\\nTraining with Tool Augmentation: LLMs are trained to inter-\\nact with diverse tools, enhancing planning abilities to overcome\\nthe limitations of zero-shot tool augmentation [211, 27, 212,\\n213]. Gorilla [211] instruction-tunes LLaMA with information\\nretrieval from API documentation. It uses the self-instruct [19]\\n18Figure 13: A basic flow diagram of tool augmented LLMs. Given an input and\\na set of available tools, the model generates a plan to complete the task. The\\ntool augmented LLMs utilize di fferent modules iteratively, such as retriever,\\ntool execution, read-write to memory, feedback, etc., depending on the task.\\ndata generation pipeline with GPT-4 by providing in-context\\nexamples retrieved from API documentation. Tool augmented\\nlanguage model (TALM) [27] fine-tunes T5 [10] for tool use\\nwith a self-play approach, where it iteratively completes tool\\nmanipulation tasks and includes them back in the training set.\\nToolLLM [213] collects 16k APIs from RapidAPI. It samples\\nAPIs from the list to generate an instruction-tuning dataset us-\\ning ChatGPT in single-tool and multi-tool scenarios. For high-\\nquality datasets, ToolLLM suggested a depth-first search-based\\ndecision tree (DFSDT) method to generate ground-truths with\\ndiverse reasoning and planning.\\nMultimodal Tool Augmentation: The compositional reasoning\\ncapacity of LLMs allows them to manipulate tools in multi-\\nmodal settings [205, 206, 214]. Following the pipeline shown\\nin Figure 13, the LLM outlines a plan, generally executing in a\\nsequence: Plan→Tool selection→Execute→Inspect→\\nGenerate, to respond to the user query. Here, the database of\\ntools is rich in modalities, including text, images, etc. Many of\\nthe multimodal tool augmentation systems employ multimodal\\nLLMs [31, 215, 214, 206], while others utilize single modality\\nLLMs and generate a plan on using di fferent modality tools to\\nsolve multimodal queries [216].\\n3.5. LLMs-Powered Agents\\nAI agents are autonomous entities, capable of planning,\\ndecision-making, and performing actions to achieve complex\\ngoals. In the early days, AI agents were rule-based, de-\\nsigned for narrow tasks, and had limited capabilities, such\\nas Clippy [217] and Deep Blue [218]. In contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it\\npossible to incorporate them in diverse applications, includ-\\ning LLMs-powered agents [214, 206], where LLMs behaveas the brain of agents. LLMs have been incorporated in web\\nagents [156, 157], coding agents [219], tool agents [27, 213],\\nembodied agents [26], and conversational agents [185], requir-\\ning minimal to no fine-tuning\". Below we summarize the re-\\nsearch in LLMs-based autonomous agents. For a more detailed\\ndiscussion, please refer to [220, 221].\\nLLMs Steering Autonomous Agents: LLMs are the cognitive\\ncontrollers of the autonomous agents. They generate plans, rea-\\nson about tasks, incorporate memory to complete tasks, and\\nadapt the outline depending on the feedback from the environ-\\nment. Depending on the acquired capabilities of LLMs, many\\nmethods fine-tune, propose a better prompting approach, or uti-\\nlize di fferent modules to enhance agents’ performance. Mod-\\nules and strategies employed in autonomous agents are briefly\\ndiscussed below.\\nPlanning and Reasoning: Completing a complex task requires\\nhuman-like logical thinking, planning necessary steps, and\\nreasoning current and future directions. Prompting methods\\nlike chain-of-thoughts [103], tree-of-thoughts [105], and self-\\nconsistency [104] are central to agents, eliciting LLMs to rea-\\nson its actions and choose among di fferent paths for task com-\\npletion. When LLMs are prompted with a task description and\\na sequence of actions, they can accurately generate plan ac-\\ntions without any fine-tuning [222]. Reasoning via planning\\n(RAP) [223] incorporates a re-purposed LLM as a world model\\nto reason about future outcomes and explore alternative paths\\nfor task completion. Retroformer [224] uses a retrospective\\nLLM to improve main LLM planning and reasoning capabil-\\nities by providing helpful task cues.\\nFeedback: LLMs in open-loop systems generate plans and as-\\nsume that the agent will complete them successfully. However,\\nthe actual scenario is di fferent with failures and variable re-\\nsponses from the environment. To correctly complete tasks,\\nmany methods use LLMs in a closed-loop where the action re-\\nsponse is provided as feedback to the LLMs to re-assess and\\nupdate the plan as required [225, 226, 227, 185]. Another di-\\nrection of research exploits LLMs as reward functions to train\\nreinforcement learning (RL) policies instead of humans [228].\\nMemory: LLMs can learn from the context provided in the\\nprompt. In addition to internal memory, various systems em-\\nploy external memory to save the response history. Reflex-\\nion [185] maintains an episodic memory to use previous re-\\nsponses as feedback to improve future decision-making. Retro-\\nformer [224] improves its responses by employing short-term\\nand long-term memory, where short-term memory contains re-\\ncent responses and long-term memory keeps summarized failed\\nattempts to add in the prompt as reflection.\\nMulti-Agents Systems: LLMs can play user-defined roles and\\nbehave like a specific domain expert. In multi-agent systems,\\neach LLM is assigned a unique role, simulating human behav-\\nior and collaborating with other agents to complete a complex\\ntask [219, 229].\\nLLMs in Physical Environment: LLMs are good at\\ninstruction-following, however, utilizing them for physically\\ngrounded tasks requires adaptation, as they lack real-world\\nknowledge. This could lead to generating illogical responses\\nfor a particular physical situation [230, 26]. SayCan [230]\\n19make LLMs aware of the available low-level task operations.\\nLLM (Say) builds a high-level plan to complete the task and\\na learned a ffordance function (Can) explores the possibility of\\nexecuting the plan in the real world. SayCan uses RL to train\\nthe language-conditioned a ffordance function. PaLM-E enables\\nthe LLM to solve grounded tasks by training multi-modal LLM\\nfeeding inputs directly from the sensors.\\nManipulation: In the area of manipulation [226, 231], LLMs\\nenhance a robot’s dexterity and adaptability, excelling in tasks\\nlike object recognition, grasping, and collaboration. They ana-\\nlyze visual and spatial information to determine the most e ffec-\\ntive approach to interact with objects.\\nNavigation: LLMs enhance a robot’s ability to navigate com-\\nplex environments with precision and adaptability [232, 233,\\n234, 235]. They generate feasible paths and trajectories for\\nrobots, accounting for intricate environmental details [236].\\nThis ability is valuable in scenarios requiring precise and\\ndynamically adaptable navigation in environments like ware-\\nhouses, transport, healthcare facilities, and residences.\\n3.6. E fficient LLMs\\nDeploying LLMs in production is expensive. Reducing their\\nrunning costs while preserving performance is an appealing\\narea of research. This section summarizes the approaches sug-\\ngested to enhance LLMs’ e fficiency.\\n3.6.1. Parameter E fficient Fine-Tuning\\nFine-tuning LLMs with tens or hundreds of billions of pa-\\nrameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\\n(540B), etc., is computationally intensive and time-consuming.\\nTo avoid complete model fine-tuning, numerous parameter-\\nefficient fine-tuning (PEFT) techniques [40, 237, 41, 38, 39] try\\nto achieve acceptable model fine-tuning performance at reduced\\ncosts. As compared to full fine-tuning [238], PEFT performs\\nbetter in low-resource setups, achieves comparable perfor-\\nmance on medium-resource scenarios, and performs worse than\\nfull fine-tuning under high-resource availability. An overview\\nof different PEFT approaches is shown in Figure 14.\\nAdapter Tuning: Adds a few trainable parameters within the\\ntransformer block. The adapter layer is a sequence of feature\\ndownscaling, non-linearity, and upscaling [106]. Variants of\\nadapter tuning inject adapter layers sequentially [106] and in\\nparallel [38], whereas the mixture of adapter (AdaMix) [239]\\nemploys multiple adapter modules in a single layer. AdaMix\\nroutes input instances randomly to one of the multiple down-\\nscale and upscale modules. The mixture of adapters is averaged\\nout for inference to avoid additional latency. Low-Rank Adap-\\ntation (LoRA) [240] learns low-rank decomposed matrices to\\nfreeze original weights. The learned weights are fused with the\\noriginal weights for inference, avoiding latency.\\nPrompt Tuning: Prompting is an e ffective way to adapt a\\npre-trained LLM for the downstream task. However, manual\\nprompts bring uncertainty in the model’s prediction, where a\\nchange in a single word drops the performance [237]. Prompt\\ntuning alleviates this problem by fine-tuning only 0.001%-3%\\nadditional parameters [241]. It concatenates trainable promptparameters with the model embeddings [237, 40, 241]. Task-\\nspecific fixed discrete prompts are concatenated with input em-\\nbeddings in [40]. As discrete prompts bring instability, prompts\\nare encoded through a learnable mapping in P-Tuning [237],\\nnaming continuous prompts, which are appended with the dis-\\ncrete prompts. Only the prompt encoder is trainable in the\\nmodel. In an extension of P-Tuning, continuous prompts are\\nconcatenated with each layer of the network in [241]. Progres-\\nsive prompts [242] avoid catastrophic forgetting and transfer\\npreviously learned knowledge by sequentially adding trainable\\nprompt embeddings to the previously frozen task embeddings.\\nPrefix Tuning: A set of trainable task-specific prefix vectors\\nare appended to the frozen transformer layers in prefix tun-\\ning [41]. The prefix vectors are virtual tokens attended by the\\ncontext tokens on the right. In addition, adaptive prefix tun-\\ning [243] applies a gating mechanism to control the information\\nfrom the prefix and actual tokens.\\nBias Tuning: Fine-tuning only bias terms in small to medium\\ntraining data has been found e ffective in BitFit [244]. This\\nmethod achieves full fine-tuning performance for tasks with less\\ntraining data and comparable performance with more training\\ndata.\\n3.6.2. Quantization\\nLLMs require extensive computing and memory for infer-\\nence. Deploying a 175B parameter GPT-3 model needs at\\nleast 5x80GB A100 GPUs and 350GB of memory to store in\\nFP16 format [44]. Such demanding requirements for deploying\\nLLMs make it harder for smaller organizations to utilize them.\\nModel compression is an e ffective solution but comes at the cost\\nof degraded performance, especially at large scales greater than\\n6B. These models exhibit very large magnitude outliers that do\\nnot exist in smaller models [245], making it challenging and re-\\nquiring specialized methods for quantizing LLMs [44, 246].\\nPost-Training Quantization: Minimal or no training is re-\\nquired in this type of quantization, without significantly com-\\npromising the model performance. LLM-8-bit [245] uses full-\\nprecision matrix multiplication for weights associated with out-\\nlier features and 8-bit for remaining features. The lower pre-\\ncision multiplication outputs are converted to FP-16 and con-\\ncatenated with others. The quantized models have homogenous\\nword embeddings, which may degrade their performance. To\\nfix this, token-level knowledge distillation is employed in [45]\\nalong with independent quantization scaling factors for each\\nmodule due to varying weight distribution. Feature distribu-\\ntions are asymmetric and appear in di fferent channels; outlier\\nsuppression [247] shifts and scales per-channel activation dis-\\ntributions for e ffective quantization. SmoothQuant [44] quan-\\ntizes activations and weights to INT8 format by smoothing\\nactivations and migrating the quantization di fficulty toward\\nweights. It multiplies the inverse of the smoothing factor with\\nweights, which introduces a few outliers in the weights but is\\neasier to quantify than unsmoothed activations. OPTQ [246]\\nuses the optimal brain compression (OBC) [248] algorithm to\\nquantize the model layer-by-layer and update weights to com-\\npensate for quantization error. To improve speed and per-\\nformance, OPTQ updates weights in arbitrary order, employs\\n20Figure 14: Illustration of parameter-e fficient fine-tuning paradigms, where xis input and his hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\\nthe adapter tuning category.\\nlazy updates, and uses better Cholesky kernels. Outlier-aware\\nweight quantization (OWQ) [249] uses the OPTQ algorithm for\\nquantization but assigns higher precision to vulnerable weights,\\ncausing outliers and lower precision for others.\\nQuantization-Aware Training: To compensate for perfor-\\nmance degradation, a quantized model is fine-tuned in\\nquantization-aware training (QAT) [250, 251, 252]. Al-\\npha Tuning quantizes the model using binary coding quan-\\ntization (BCQ) [253] and fine-tunes only quantization scal-\\ning factors. This approach improves performance over\\nparameter-e fficient fine-tuning of the pre-trained model. Sim-\\nilarly, parameter-e fficient and quantization-aware adaptation\\n(PEQA) [254] reduces the precision of fully-connected layers\\nand fine-tunes only quantization scaling parameters. LLM-\\nQAT [252] generates training data from the pre-trained network\\nand trains a quantized student model with knowledge distilla-\\ntion. QLoRA [251] fine-tunes 4-bit quantized pre-trained LLM\\nwith LoRA [240] using a 4-bit normal float, which shows better\\nperformance over a 4-bit integer and float.\\n3.6.3. Pruning\\nPruning is an alternative approach to quantization to com-\\npress model size, thereby reducing LLMs deployment costs\\nsignificantly. Compared to task-agnostic pruning, task-specific\\npruning is easily achievable with good performance, where a\\nmodel is fine-tuned on the downstream task and pruned for\\nfaster inference. It is possible to prune LLMs for individual\\ntasks, but the cost of pruning and deploying task-specific mod-\\nels is high. To overcome this, many structured and unstructured\\npruning methods for LLMs have been proposed to maintain rea-\\nsonable performance across all tasks while shrinking the model\\nsize [255, 42, 256].\\nUnstructured Pruning: This kind of pruning removes less im-\\nportant weights without maintaining any structure. Existing\\nLLM pruning methods take advantage of the unique charac-\\nteristics of LLMs, uncommon for smaller models, where a\\nsmall subset of hidden states are activated with large magni-\\ntude [245]. Pruning by weights and activations (Wanda) [255]\\nprunes weights in every row based on importance, calculated\\nby multiplying the weights with the norm of input. The prunedmodel does not require fine-tuning, thereby saving computa-\\ntional costs. Outlier weighed layerwise sparsity (OWL) [257]\\nextends Wanda with non-uniform layer pruning. It shows that\\nthe number of outliers varies for di fferent layers; therefore, the\\nmodel should have variable pruning ratios for better perfor-\\nmance for every layer. Contrastive pruning (CAP) [43] itera-\\ntively prunes the model by training the sparse model using con-\\ntrastive loss between pre-trained, fine-tuned, and snapshots of\\nprevious sparse models to learn task-specific and task-agnostic\\nknowledge.\\nStructured Pruning: Here, the parameters are removed in\\ngroups, rows, columns, or matrices, which speeds up the\\ninference because of e ffective hardware tensor core utiliza-\\ntion [255]. LLM-Pruner [42] employs a 3-stage structured\\npruning strategy, identifying the groups of hidden states caus-\\ning each other to activate during the forward-pass, keeping im-\\nportant groups and removing less important ones, and fine-\\ntuning the pruned model with LoRA. Sparsity-induced mask\\nlearning (SIMPLE) [258] prunes the network using learnable\\nmasks. Similarly, another method prunes LLMs by learning\\nmasks and removing unimportant rank-1 components of the\\nfactorized weight matrix [256].\\n3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-\\ning applications, an increasing number of research works are\\nnow facilitating LLMs to perceive di fferent modalities of infor-\\nmation like image [259, 260, 261], video [262, 263, 264], au-\\ndio [265, 264, 266], etc. Multimodal LLMs (MLLMs) present\\nsubstantial benefits compared to standard LLMs that process\\nonly text. By incorporating information from various modal-\\nities, MLLMs can achieve a deeper understanding of context,\\nleading to more intelligent responses infused with a variety of\\nexpressions. Importantly, MLLMs align closely with human\\nperceptual experiences, leveraging the synergistic nature of our\\nmultisensory inputs to form a comprehensive understanding of\\nthe world [266, 26]. Coupled with a user-friendly interface,\\nMLLMs can o ffer intuitive, flexible, and adaptable interactions,\\nallowing users to engage with intelligent assistants through a\\nspectrum of input methods. According to the ways of construct-\\n21ing models, current MLLMs can be generally divided into three\\nstreams: pre-training, fine-tuning, and prompting. In this sec-\\ntion, we will discuss more details of these main streams, as well\\nas the important application of MLLMs in visual reasoning.\\nPre-training: This stream of MLLMs intends to support di ffer-\\nent modalities using unified end-to-end models. For instance,\\nFlamingo [259] applies gated cross-attention to fuse vision and\\nlanguage modalities, which are collected from pre-trained and\\nfrozen visual encoder and LLM, respectively. Moreover, BLIP-\\n2 [260] proposes a two-stage strategy to pre-train a Querying\\nTransformer (Q-Former) for the alignment between vision and\\nlanguage modalities: in the first stage, vision-language repre-\\nsentation learning is bootstrapped from a frozen visual encoder;\\nand in the second stage, a frozen LLM bootstraps vision-to-\\nlanguage generative learning for zero-shot image-to-text gen-\\neration. Similarly, MiniGPT-4 [267] deploys pre-trained and\\nfrozen ViT [268], Q-Former and Vicuna LLM [149], only train-\\ning the linear projection layer for vision and language modali-\\nties alignment.\\nFine-tuning: Derived from instruction tuning [16] for NLP\\ntasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\\nusing multimodal instructions. Following this method, LLMs\\ncan be easily and e ffectively extended as multimodal chat-\\nbots [267, 261, 29] and multimodal task solvers [269, 30, 270].\\nThe key issue of this stream of MLLMs is to collect multi-\\nmodal instruction-following data for fine-tuning [58]. To ad-\\ndress this issue, the solutions of benchmark adaptation [269,\\n271, 272], self-instruction [19, 31, 273], and hybrid composi-\\ntion [274, 270] are employed, respectively. To mitigate the gap\\nbetween the original language modality and additional modal-\\nities, the learnable interface is introduced to connect di ffer-\\nent modalities from frozen pre-trained models. Particularly,\\nthe learnable interface is expected to work in a parameter-\\nefficient tuning manner: e.g., LLaMA-Adapter [275] applies\\nan efficient transformer-based adapter module for training,\\nand LaVIN [274] dynamically learns the multimodal feature\\nweights using a mixture-of-modality adapter. Di fferent from\\nthe learnable interface, the expert models can directly convert\\nmultimodalities into language: e.g., VideoChat-Text [262] in-\\ncorporates Whisper [276], a speech recognition expert model,\\nto generate the captions of given videos for the understanding\\nof following LLMs.\\nPrompting: Different from the fine-tuning technique that\\ndirectly updates the model parameters given task-specific\\ndatasets, the prompting technique provides certain context, ex-\\namples, or instructions to the model, fulfilling specialized tasks\\nwithout changing the model parameters. Since prompting can\\nsignificantly reduce the need for large-scale multimodal data,\\nthis technique is widely used to construct MLLMs. Particularly,\\nto solve multimodal Chain of Thought (CoT) problems [103],\\nLLMs are prompted to generate both the reasoning process and\\nthe answer given multimodal inputs [277]. On this front, di ffer-\\nent learning paradigms are exploited in practice: for example,\\nMultimodal-CoT [277] involves two stages of rationale genera-\\ntion and answer inference, where the input of the second stage\\nis a combination of the original input and the output of the first\\nstage; and CoT-PT [278] applies both prompt tuning and spe-cific visual bias to generate a chain of reasoning implicitly. In\\naddition to CoT problems, LLMs can also be prompted with\\nmultimodal descriptions and tools, e ffectively dividing complex\\ntasks into sub-tasks [279, 280].\\nVisual Reasoning Application: Recent visual reasoning sys-\\ntems [281, 282, 206, 283] tend to apply LLMs for better visual\\ninformation analysis and visual-language integration. Di ffer-\\nent from previous works [284, 285] that rely on limited VQA\\ndatasets and small-scale neural networks, current LLM-aided\\nmethods o ffer benefits of stronger generalization ability, emer-\\ngent ability, and interactivity [58]. To realize visual reasoning\\nwith the help of LLMs, prompting and fine-tuning techniques\\ncan also be utilized: for example, PointClip V2 [282] applies\\nLLMs to generate 3D-specific prompts, which are encoded as\\ntextual features and then combined with visual features for\\n3D recognition; and GPT4Tools [31] employs LoRA [240] to\\nfine-tune LLMs following tool-related instructions. Serving\\nas a controller [283], decision maker [286], or semantics re-\\nfiner [281, 287], LLMs significantly facilitates the progress of\\nvisual reasoning research.\\n3.8. Summary and Discussion\\n3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-\\ntecture and training strategies have a big impact on performance\\nand stability. Here, we summarize key architectural modules\\nused in various LLMs, leading to better performance, reduced\\ntraining time and memory, and better training stability.\\nLayer Normalization: The performance and training stability\\nof LLMs are a ffected significantly by layer normalization. Pre-\\nnorm, that is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [6, 127, 108].\\nBLOOM [13] and AlexaTM [122] utilize an additional layer\\nnormalization before embedding layer to stabilize the training\\nof large-scale models, while the model’s zero-shot generaliza-\\ntion ability can be negatively impacted [13]. However, another\\nstudy [33] finds that pre-norm degrades fine-tuned model per-\\nformance as compared to post-norm, and there are no stability\\nbenefits of pre-norm beyond the 100B scale. Therefore, GLM-\\n130B [33] used deep-norm which is a variant of post-norm for\\nbetter downstream task performance after fine-tuning.\\nPositional Encoding: Like other building blocks of the model,\\npositional encoding also a ffects the performance and training\\nstability of LLMs. BLOOM [13] finds ALiBi outperforms\\nlearned and rotary positional encodings. Contrary to this,\\nGLM-130B [33] identifies rotary positional encoding as being\\nbetter than ALiBi. So, there is no conclusion in the literature\\nabout positional encodings yet.\\nParallel Attention: In this type of attention, feed-forward and\\nattention layers are parallel to each other rather than sequen-\\ntial in a transformer block. It has been shown to reduce train-\\ning time by 15%. There is no evidence of performance drop\\ndue to this change in the literature and it is used by the models\\nPaLM [15], GPT-NeoX [118], and CodeGen [130].\\nMulti-Query Attention It has shared key and value attention\\nheads in a transformer block while query attention heads are\\n22projected as usual. This reduces memory usage and speeds up\\nsampling in autoregressive decoding. No performance degrada-\\ntion has been observed with this change and it makes the train-\\ning efficient allowing larger batch sizes. Multi-query attention\\nis used in [15, 132].\\nMixture of Experts: This type of architecture enables eas-\\nily scaling models to trillions of parameters [92, 91]. Only a\\nfew experts are activated during the computation making them\\ncompute-e fficient. The performance of MoE models is better\\nthan dense models for the same amount of data and requires less\\ncomputation during fine-tuning to achieve performance similar\\nto dense models as discussed in [91]. MoE architectures are\\nless prone to catastrophic forgetting, therefore are more suited\\nfor continual learning [92]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [92].\\nSparse vs Dense Activated: GPT-3 [6] uses sparse transform-\\ners [67] whereas GLaM [91] and PanGu-P[92] use MoE [121]\\narchitectures to lower computational costs and increase the\\nmodel size and capacity. According to the literature, sparse\\nmodules do not degrade the model’s performance [67]. How-\\never, more experiments are required to verify this statement.\\n3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-\\ning costs, avoid loss divergence, and achieve better perfor-\\nmance. We summarize and discuss some of these key tricks\\nused in di fferent LLMs.\\nMixed Precision: It is a famous method for LLMs to reduce\\nmemory usage and improve training e fficiency. In mixed pre-\\ncision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept\\nin FP32 format [120]. A drawback associated with this for-\\nmat change is training instability due to a smaller value range\\nresulting in loss spikes [33]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs precision-\\nsensitive operations like gradient accumulation and softmax in\\nFP32 [13]. BF16 has better performance and training stability\\nbut uses more memory and is supported on specific hardware,\\nfor example, A100 GPUs. Therefore, its adoption in LLMs is\\nlimited.\\nTraining Instability: Loss divergence or spiking is a common\\nissue in LLMs that occurs multiple times during training. This\\nhappens in the presence of gradient clipping [15]. To mitigate\\nthis problem, many approaches suggest restarting training from\\nan earlier checkpoint [15, 33, 91], skipping 200-500 earlier\\ndata batches at the point of divergence in [15] and re-shu ffling\\nbatches in [91]. The embedding layer gradient shrink proves to\\nfurther stabilize the training as its gradient norm is significantly\\nlarger than the other layers [33]. Another suggestion to improve\\ntraining stability for larger models is not to use biases in dense\\nand norm layers as in [15].\\nWeight Initialization: It plays a significant role in model con-\\nvergence and training stability. GPT-NeoX [118] initializes\\nfeed-forward layers before residuals with2\\nL√\\ndas in [143] and\\nother layers with the small initialization scheme [288]. This\\navoids activations growing exponentially with increasing depth.MT-NLG [117] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [288]. Various models perform random weight initial-\\nization which can cause bad initialization, Galactica [138] sug-\\ngests a longer warmup to negate the e ffect.\\nLearning Rate: A suitable learning rate is important for sta-\\nble training. It is suggested to use a lower value [13, 15, 124]\\nwith warmup and decay (cosine or linear). Usually, the learn-\\ning rate is within the range 1 e−4to 8e−4. Moreover, MT-NLG\\n(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\\ning learning rates based on the model size using the GPT-3 [6]\\nmodels ranging between 13B and 175B. This avoids tuning the\\nlearning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,\\npipeline, and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\\nIn addition to 3D parallelism, BLOOM [13] uses a zero op-\\ntimizer [37] to shard optimizer states. PanGu- α[108] and\\nPanGu- Σ[92] go beyond 3D parallelism and apply 5D paral-\\nlelism which additionally contains optimizer parallelism and\\nrematerialization.\\nMode Switching: It adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve downstream task performance\\nin [125, 124, 122]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation: Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-3 [6]\\nand other LLMs use in-context learning to control generated\\ntext. While in-context learning helps in controlling the gener-\\nated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\\nto rank its generated text for credibility and soft prompts such as\\ngenre, topic, keywords, sentiment, and length for better control\\non generated text.\\n3.8.3. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing di-\\nverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks by\\na large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance di fference between zero-shot and few-shot is\\nlarge for pre-trained models [6, 15], naming LLMs as meta-\\nlearners [6]. LLMs zero-shot evaluations underperform unsu-\\npervised methods in neural machine translation [6]. The liter-\\nature shows pre-training is not enough for good zero-shot per-\\nformance [15, 16]. To improve the zero-shot performance the\\nliterature suggests using instruction fine-tuning that improves\\nthe zero-shot performance significantly and outperforms base-\\nlines. Instruction fine-tuning has also been shown to improve\\nzero-shot generalization to unseen tasks. Another model, Flan-\\nPaLM [16], unlocks zero-shot reasoning with CoT training.\\n233.8.5. Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for di fferent\\ntasks, for example, encoder-only for NLU tasks, decoder-only\\nfor NLG, and encoder-decoder for sequence2sequence model-\\ning. Encoder-only models are famous for smaller models such\\nas Bert [7], RoBERTa [289], etc., whereas LLMs are either\\ndecoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\\nWhile decoder-only models are good at NLG tasks, various\\nLLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\\nLLaMA [146], are decoder-only models with significant per-\\nformance gains on both NLU and NLG tasks. In contradic-\\ntion to this, T5 [10] and UL2 [125] identify encoder-decoder\\nmodels out-performing decoder-only models. In another study,\\nPaLM [15] finds increasing the size of decoder-only models\\ncan reduce the performance gap between decoder-only and\\nencoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [125, 122] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5 +[34]\\nuses an encoder-decoder architecture with multiple training ob-\\njectives for di fferent tasks, activating the encoder, decoder, or\\nboth according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in di ffer-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\n4. Model Configurations\\nWe provide di fferent statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,\\nparallelism, etc in Table 3 and Table 4. Architecture details\\nof pre-trained LLMs are available in Table 5. Providing these\\ndetails for instruction-tuned models is unnecessary because it\\nfine-tunes pre-trained models for instruction datasets. Hence,\\narchitectural details are the same as the baselines. Moreover,\\noptimization settings for various LLMs are available in Table 6\\nand Table 7. We do not include details on precision, warmup,\\nand weight decay in Table 7. These details are not as important\\nas others to mention for instruction-tuned models, and are not\\nprovided by the papers.\\n5. Datasets and Evaluation\\nGenerating training and evaluation datasets is expensive be-\\ncause of the large-scale data demand of LLMs. Hence, datasets\\nfor training and benchmarking these models are topics of key\\nimportance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\\nThe performance of LLMs largely depends on the training\\ndata’s quality, size, and diversity. Preparing training datasets\\nof high quality at a large scale is laborious. Researchers havesuggested various pre-training and fine-tuning datasets to en-\\nhance LLMs capabilities. We summarize these e fforts in Ta-\\nble 8. While numerous training datasets are available in the\\nliterature, we cover the most widely used ones in our summary.\\n5.2. Evaluation Datasets and Tasks\\nThe evaluation of LLMs is important in gauging their profi-\\nciency and limitations. This process measures the model’s abil-\\nity to comprehend, generate, and interact with human language\\nacross a spectrum of tasks. Evaluating a language model (LM)\\nis divided into two broader categories: 1) natural language un-\\nderstanding (NLU) and 2) natural language generation (NLG).\\nIt is emphasized that tasks in NLU and NLG are softly catego-\\nrized and are often used interchangeably in the literature.\\nNatural Language Understanding: This task measures the lan-\\nguage understanding capacity of LMs. It encompasses multiple\\ntasks, including sentiment analysis, text classification, natural\\nlanguage inference (NLI), question answering (QA), common-\\nsense reasoning (CR), mathematical reasoning (MR), reading\\ncomprehension (RC), etc.\\nNatural Language Generation: This task assesses the language\\ngeneration capabilities of LLMs by understanding the provided\\ninput context. It includes tasks such as summarization, sen-\\ntence completion, machine translation (MT), dialogue genera-\\ntion, etc.\\nNumerous datasets are proposed for each task, evaluating\\nLLMs against di fferent characteristics. To provide an overview\\nof evaluation datasets, we briefly discuss a few famous datasets\\nwithin each category and o ffer a comprehensive list of datasets\\nin Table 9. Moreover, we show a detailed overview of the train-\\ning datasets and evaluation tasks and benchmarks used by vari-\\nous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\\nble 11. We also compare the top-performing LLMs in various\\nNLP tasks in Table 12.\\n5.2.1. Multi-task\\nMMLU [297]: A benchmark that measures the knowledge\\nacquired by models during pretraining and evaluates models in\\nzero-shot and few-shot settings across 57 subjects, testing both\\nworld knowledge and problem-solving ability.\\nSuperGLUE [2]: A more challenging and diverse successor\\nto the GLUE [299] benchmark, SuperGLUE includes a variety\\nof language understanding tasks, such as question answering,\\nnatural language inference, and co-reference resolution. It is\\ndesigned to provide a rigorous test of language understanding\\nand requires significant progress in areas like sample-e fficient,\\ntransfer, multi-task, and unsupervised or self-supervised learn-\\ning.\\nBIG-bench [298]: The BIG-bench (Behavior of Intelligent\\nGenerative Models Benchmark) is a large-scale benchmark de-\\nsigned to test the abilities of LLMs across a wide range of\\ntasks, including reasoning, creativity, ethics, and understanding\\nof specific domains.\\nGLUE [299]: The General Language Understanding Evalua-\\ntion (GLUE) benchmark is a collection of resources for train-\\ning, evaluating, and analyzing natural language understanding\\n24Table 3: Summary of pre-trained LLMs ( >10B). Only the LLMs discussed individually in the previous sections are summarized. “Data /Tokens” is the model’s\\npre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\\n(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs /TPUs\\nhourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\\nre-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\\n(T), pipeline parallelism (P), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column, “DS” is a short form for\\nDeep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\\nModelsPublication\\nVenueLicense\\nTypeModel\\nCreators PurposeNo. of\\nParamsCommercial\\nUseSteps\\nTrainedData /\\nTokensData\\nCleaningNo. of\\nProcessing UnitsProcessing\\nUnit TypeTraining\\nTimeCalculated\\nTrain. CostTraining\\nParallelism Library\\nT5 [10] JMLR\\'20 Apache-2.0 Google General 11B ✓ 1M 1T Heur+Dedup 1024 TPU v3 - - D+M Mesh TensorFlow\\nGPT-3 [6] NeurIPS\\'20 - OpenAI General 175B× - 300B Dedup +QF - V100 - - M -\\nmT5 [11] NAACL\\'21 Apache-2.0 Google General 13B ✓ 1M 1T - - - - - - -\\nPanGu-α[108] arXiv\\'21 Apache-2.0 Huawei General 200B ✓ 260k 1.1TB Heur+Dedup 2048 Ascend 910 - - D+OP+P+O+R MindSpore\\nCPM-2 [12] AI Open\\'21 MIT Tsinghua General 198B ✓ 1M 2.6TB Dedup - - - - D+M JAXFormer\\nCodex [131] arXiv\\'21 - OpenAI Coding 12B× - 100B Heur - - - - - -\\nERNIE 3.0 [110] arXiv\\'21 - Baidu General 10B× 120k∗375B Heur+Dedup 384 V100 - - M∗PaddlePaddle\\nJurassic-1 [112] White-Paper\\'21 Apache-2.0 AI21 General 178B ✓ - 300B - 800 GPU - - D+M+P Megatron +DS\\nHyperCLOV A [114] EMNLP\\'21 - Naver General 82B× - 300B Clf+Dedup +PF 1024 A100 321h 1.32 Mil M Megatron\\nYuan 1.0 [115] arXiv\\'21 Apache-2.0 - General 245B ✓ 26k∗180B Heur+Clf+Dedup 2128 GPU - - D+T+P -\\nGopher [116] arXiv\\'21 - Google General 280B× - 300B QF+Dedup 4096 TPU v3 920h 13.19 Mil D+M JAX+Haiku\\nERNIE 3.0 Titan [35] arXiv\\'21 - Baidu General 260B× - 300B Heur+Dedup - Ascend 910 - - D+M+P+D* PaddlePaddle\\nGPT-NeoX-20B [118] BigScience\\'22 Apache-2.0 EleutherAI General 20B ✓ 150k 825GB None 96 40G A100 - - M Megatron +DS+PyTorch\\nOPT [14] arXiv\\'22 MIT Meta General 175B ✓ 150k 180B Dedup 992 80G A100 - - D+T Megatron\\nBLOOM [13] arXiv\\'22 RAIL-1.0 BigScience General 176B ✓ - 366B Dedup +PR 384 80G A100 2520h 3.87 Mil D+T+P Megatron +DS\\nGalactica [138] arXiv\\'22 Apache-2.0 Meta Science 120B× 225k 106B Dedup 128 80GB A100 - - - Metaseq\\nGLaM [91] ICML\\'22 - Google General 1.2T× 600k∗600B Clf 1024 TPU v4 - - M GSPMD\\nLaMDA [140] arXiv\\'22 - Google Dialog 137B× 3M 2.81T Filtered 1024 TPU v3 1384h 4.96 Mil D+M Lingvo\\nMT-NLG [117] arXiv\\'22 Apache-v2.0 MS.+Nvidia General 530B× - 270B - 4480 80G A100 - - D+T+P Megatron +DS\\nAlphaCode [132] Science\\'22 Apache-v2.0 Google Coding 41B ✓ 205k 967B Heur+Dedup - TPU v4 - - M JAX+Haiku\\nChinchilla [96] arXiv\\'22 - Google General 70B× - 1.4T QF+Dedup - TPUv4 - - - JAX+Haiku\\nPaLM [15] arXiv\\'22 - Google General 540B× 255k 780B Heur 6144 TPU v4 - - D+M JAX+T5X\\nAlexaTM [122] arXiv\\'22 Apache v2.0 Amazon General 20B× 500k 1.1T Filtered 128 A100 2880h 1.47 Mil M DS\\nU-PaLM [124] arXiv\\'22 - Google General 540B× 20k - - 512 TPU v4 120h 0.25 Mil - -\\nUL2 [125] ICLR\\'23 Apache-2.0 Google General 20B ✓ 2M 1T - 512 TPU v4 - - M JAX+T5X\\nGLM [33] ICLR\\'23 Apache-2.0 Multiple General 130B× - 400B - 768 40G A100 1440h 3.37 Mil M -\\nCodeGen [130] ICLR\\'23 Apache-2.0 Salesforce Coding 16B ✓ 650k 577B Heur+Dedup - TPU v4 - - D+M JAXFormer\\nLLaMA [127] arXiv\\'23 - Meta General 65B× 350k 1.4T Clf+Heur+Dedup 2048 80G A100 504h 4.12 Mil D+M xFormers\\nPanGu Σ[92] arXiv\\'23 - Huawei General 1.085T× - 329B - 512 Ascend 910 2400h - D+OP+P+O+R MindSpore\\nBloombergGPT [141] arXiv23 - Bloomberg Finance 50B× 139k 569B Dedup 512 40G A100 1272h 1.97 Mil M PyTorch\\nXuan Yuan 2.0 [142] arXiv23 RAIL-1.0 Du Xiaoman Finance 176B ✓ - 366B Filtered 80GB A100 - - P DS\\nCodeT5 +[34] arXiv\\'23 BSD-3 Salesforce Coding 16B ✓ 110k 51.5B Dedup 16 40G A100 - - - DS\\nStarCoder [137] arXiv\\'23 OpenRAIL-M BigCode Coding 15.5B ✓ 250k 1T Dedup +QF+PF 512 80G A100 624h 1.28 Mil D+T+P Megatron-LM\\nLLaMA-2 [21] arXiv\\'23 LLaMA-2.0 Meta General 70B ✓ 500k 2T Minimal Filtering - 80G A100 1.7Mh - - -\\nPaLM-2 [123] arXiv\\'23 - Google General -× - -Ddedup +PF+QF - - - - - -\\nTable 4: Summary of instruction tuned LLMs ( >10B). All abbreviations are the same as Table 3. Entries in “Data /Tokens” starting with “S-” represents the number\\nof training samples.\\nModelsPublication\\nVenueLicense\\nTypeModel\\nCreators PurposeNo. of\\nParamsCommercial\\nUsePre-trained\\nModelsSteps\\nTrainedData /\\nTokensNo. of\\nProcessing UnitsProcessing\\nUnit TypeTrain.\\nTimeCalculated\\nTrain. CostTrain.\\nParallelism Library\\nWebGPT [156] arXiv\\'21 - OpenAI General 175B× GPT-3 - - - - - - - -\\nT0 [17] ICLR\\'22 Apache-2.0 BigScience General 11B ✓ T5 - 250B 512 TPU v3 270h 0.48 Mil - -\\nTk-Instruct [18] EMNLP\\'22 MIT AI2+ General 11B ✓ T5 1000 - 256 TPU v3 4h 0.0036 Mil - Google T5\\nOPT-IML [97] arXiv\\'22 - Meta General 175B× OPT 8k 2B 128 40G A100 - - D+T Megatron\\nFlan-U-PaLM [16] ICLR\\'22 Apache-2.0 Google General 540B ✓ U-PaLM 30k - 512 TPU v4 - - - JAX+T5X\\nmT0 [144] ACL\\'23 Apache-2.0 HuggingFace +General 13B ✓ mT5 - - - - - - - -\\nSparrow [157] arXiv\\'22 - Google Dialog 70B× Chinchilla - - 64 TPU v3 - - M -\\nWizardCoder [154] arXiv\\'23 Apache-2.0 HK Bapt. Coding 15B× StarCoder 200 S-78k - - - - - -\\nAlpaca [148] Github\\'23 Apache-2.0 Stanford General 13B ✓ LLaMA 3-Epoch S-52k 8 80G A100 3h 600 FSDP PyTorch\\nVicuna [149] Github\\'23 Apache-2.0 LMSYS General 13B ✓ LLaMA 3-Epoch S-125k - - - - FSDP PyTorch\\nLIMA [175] arXiv\\'23 - Meta+ General 65B - LLaMA 15-Epoch S-1000 - - - - - -\\nKoala [290] Github\\'23 Apache-2.0 UC-Berkley General 13B× LLaMA 2-Epoch S-472k 8 A100 6h 100 - JAX/FLAX\\nsystems. It includes a variety of tasks that test a wide range of\\nlinguistic phenomena, making it a comprehensive tool for eval-\\nuating language understanding in AI.\\n5.2.2. Language Understanding\\nWinoGrande [344]: A large-scale dataset inspired by the orig-\\ninal Winograd [347] Schema Challenge tests models on their\\nability to resolve pronoun ambiguity and encourages the devel-\\nopment of models that understand the broad context in natural\\nlanguage text.\\nCoQA [306]: A conversational question-answering dataset,\\nCoQA challenges models with questions that rely on conver-\\nsation history and require free-form text answers. Its diversecontent from seven domains makes it a rigorous test for mod-\\nels’ ability to handle a wide range of topics and conversational\\ncontexts.\\nWiC [307]: This dataset assesses a model’s ability to dis-\\ncern word meanings based on context, aiding in tasks related\\nto Word Sense Disambiguation.\\nWikitext103 [308]: With over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as lan-\\nguage modeling and translation.\\nPG19 [309]: This is a digital library of diverse books from\\nProject Gutenberg. It is specifically designed to facilitate re-\\nsearch in unsupervised learning and language modeling, with a\\n25Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\\nsize of hidden states.\\nModels TypeTraining\\nObjectiveAttention Vocab Tokenizer Norm PE Activation Bias nL nH HS\\nT5 (11B) Enc-Dec Span Corruption Standard 32k SentencePiece Pre-RMS Relative ReLU× 24 128 1024\\nGPT3 (175B) Causal-Dec Next Token Dense +Sparse - - Layer Learned GeLU ✓ 96 96 12288\\nmT5 (13B) Enc-Dec Span Corruption Standard 250k SentencePiece Pre-RMS Relative ReLU - - - -\\nPanGu-α(200B) Causal-Dec Next Token Standard 40k BPE Layer - - - 64 128 16384\\nCPM-2 (198B) Enc-Dec Span Corruption Standard 250k SentencePiece Pre-RMS Relative ReLU - 24 64 -\\nCodex (12B) Causal-Dec Next Token Standard - BPE+ Pre-Layer Learned GeLU - 96 96 12288\\nERNIE 3.0 (10B) Causal-Dec Next Token Standard - WordPiece Post-Layer Relative GeLU - 48 64 4096\\nJurassic-1 (178B) Causal-Dec Next Token Standard 256k SentencePiece∗Pre-Layer Learned GeLU ✓ 76 96 13824\\nHyperCLOV A (82B) Causal-Dec Next Token Dense +Sparse - BPE* Pre-Layer Learned GeLU - 64 80 10240\\nYuan 1.0 (245B) Causal-Dec Next Token Standard - - - - - - 76 -16384\\nGopher (280B) Causal-Dec Next Token Standard 32k SentencePiece Pre-RMS Relative GeLU ✓ 80 128 16384\\nERNIE 3.0 Titan (260B) Causal-Dec Next Token Standard - WordPiece Post-Layer Relative GeLU - 48 192 12288\\nGPT-NeoX-20B Causal-Dec Next Token Parallel 50k BPE Layer Rotary GeLU ✓ 44 64 -\\nOPT (175B) Causal-Dec Next Token Standard - BPE - - ReLU ✓ 96 96 -\\nBLOOM (176B) Causal-Dec Next Token Standard 250k BPE Layer ALiBi GeLU ✓ 70 112 14336\\nGalactica (120B) Causal-Dec Next Token Standard 50k BPE+custom Layer Learned GeLU× 96 80 10240\\nGLaM (1.2T) MoE-Dec Next Token Standard 256k SentencePiece Layer Relative GeLU ✓ 64 128 32768\\nLaMDA (137B) Causal-Dec Next Token Standard 32k BPE Layer Relative GeGLU - 64 128 8192\\nMT-NLG (530B) Causal-Dec Next Token Standard 50k BPE Pre-Layer Learned GeLU ✓ 105 128 20480\\nAlphaCode (41B) Enc-Dec Next Token Multi-query 8k SentencePiece - - - - 64 128 6144\\nChinchilla (70B) Causal-Dec Next Token Standard 32k SentencePiece-NFKC Pre-RMS Relative GeLU ✓ 80 64 8192\\nPaLM (540B) Causal-Dec Next Token Parallel +Multi-query 256k SentencePiece Layer RoPE SwiGLU×118 48 18432\\nAlexaTM (20B) Enc-Dec Denoising Standard 150k SentencePiece Pre-Layer Learned GeLU ✓ 78 32 4096\\nSparrow (70B) Causal-Dec Pref.&Rule RM - 32k SentencePiece-NFKC Pre-RMS Relative GeLU ✓ 16∗64 8192\\nU-PaLM (540B) Non-Causal-Dec MoD Parallel +Multi-query 256k SentencePiece Layer RoPE SwiGLU×118 48 18432\\nUL2 (20B) Enc-Dec MoD Standard 32k SentencePiece - - - - 64 16 4096\\nGLM (130B) Non-Causal-Dec AR Blank Infilling Standard 130k SentencePiece Deep RoPE GeGLU ✓ 70 96 12288\\nCodeGen (16B) Causal-Dec Next Token Parallel - BPE Layer RoPE - - 34 24 -\\nLLaMA (65B) Causal-Dec Next Token Standard 32k BPE Pre-RMS RoPE SwiGLU - 80 64 8192\\nPanGu- Σ(1085B) Causal-Dec Next Token Standard - BPE Fused Layer - FastGeLU - 40 40 5120\\nBloombergGPT (50B) Causal-Dec Next Token Standard 131k Unigram Layer ALiBi GeLU ✓ 70 40 7680\\nXuan Yuan 2.0 (176B) Causal-Dec Next Token Self 250k BPE Layer ALiBi GeLU ✓ 70 112 14336\\nCodeT5 +(16B) Enc-Dec SC+NT+Cont. +Match Standard - Code-Specific - - - - - - -\\nStarCoder (15.5B) Causal-Dec FIM Multi-query 49k BPE - Learned - - 40 48 6144\\nLLaMA (70B) Causal-Dec Next Token Grouped-query 32k BPE Pre-RMS RoPE SwiGLUE - - - -\\nPaLM-2 - MoD Parallel - - - - - - - - -\\nspecial focus on long-form content.\\nC4 [10]: A clean, multilingual dataset, C4 o ffers billions of to-\\nkens from web-crawled data. It is a comprehensive resource for\\ntraining advanced Transformer models on various languages.\\nLCQMC [310]: The Large-scale Chinese Question Matching\\nCorpus (LCQMC) is a dataset for evaluating the performance\\nof models in semantic matching tasks. It contains pairs of ques-\\ntions in Chinese and their matching status, making it a valuable\\nresource for research in Chinese language understanding.\\n5.2.3. Story Cloze and Sentence Completion\\nStoryCloze [324]: It introduces a new “StoryCloze Test”, a\\ncommonsense reasoning framework for evaluating story under-\\nstanding, generation, and script learning. It considers a model’s\\nability to understand and generate coherent and sensible stories.\\nLAMBADA [325]: This dataset evaluates contextual text un-\\nderstanding through a word prediction task. Models must pre-\\ndict the last word of a passage, which is easy for humans when\\ngiven the whole passage, but not when given only the last sen-\\ntence.\\n5.2.4. Physical Knowledge and World Understanding\\nPIQA [330]: A dataset that probes the physical knowledge of\\nmodels, aiming to understand how well they are learning about\\nthe real world.\\nTriviaQA [331]: A dataset that tests models on reading com-\\nprehension and open domain question answering (QA) tasks,\\nwith a focus on Information Retrieval (IR)-style QA.ARC [332]: A larger version of the ARC-Challenge, this\\ndataset contains both easy and challenging grade-school level,\\nmultiple-choice science questions. It is a comprehensive test of\\na model’s ability to understand and answer complex questions.\\nARC-Easy [332]: A subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\nIt is a great starting point for models beginning to explore ad-\\nvanced question-answering.\\nARC-Challenge [332]: A rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval, test-\\ning the true comprehension capabilities of models.\\n5.2.5. Contextual Language Understanding\\nRACE [337]: The RACE dataset is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge\\nof a real-world examination.\\nRACE-Middle [337]: Another subset of the RACE [337]\\ndataset, RACE-Middle, contains middle school-level English\\nexam questions. It o ffers a slightly less challenging but academ-\\nically oriented evaluation of a model’s comprehension skills.\\nRACE-High [337]: A subset of the RACE [337] dataset,\\nRACE-High consists of high school-level English exam ques-\\ntions. It is designed to evaluate the comprehension ability of\\nmodels in a more academic and challenging context.\\n26Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\\nfor most of the LLMs.\\nSequence LR Optimizers Precision Weight Grad\\nModels Batch Size Length LR Warmup Decay AdaFactor Adam AdamW FP16 BF16 Mixed Decay Clip Dropout\\nT5 (11B) 211512 0.01× inverse square root ✓ - - - - - ✓\\nGPT3 (175B) 32K - 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nmT5 (13B) 1024 1024 0.01 - inverse square root ✓ - - - - - ✓\\nPanGu-α(200B) - 1024 2e-5 - - - - - -✓ - - - -\\nCPM-2 (198B) 1024 1024 0.001 - - ✓ - - - - - ✓\\nCodex (12B) - - 6e-5 ✓ cosine ✓ ✓ ✓ - -\\nERNIE 3.0 (12B) 6144 512 1e-4 ✓ linear ✓ - - - ✓ - -\\nJurassic-1 (178B) 3.2M 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nHyperCLOV A (82B) 1024 - 6e-5 - cosine ✓ - - - ✓ - -\\nYuan 1.0 (245B) <10M 2048 1.6e-4 ✓ cosine decay to 10% ✓ - - - ✓ - -\\nGopher (280B) 3M 2048 4e-5 ✓ cosine decay to 10% ✓ ✓ - ✓ -\\nERNIE 3.0 Titan (260B) - 512 1e-4 ✓ linear ✓ ✓ ✓ ✓ -\\nGPT-NeoX-20B 1538 2048 0.97e-5 ✓ cosine ✓ ✓ ✓ ✓×\\nOPT (175B) 2M 2048 1.2e-4 - linear ✓ ✓ ✓ ✓ ✓\\nBLOOM (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓×\\nGalactica (120B) 2M 2048 7e-6 ✓ linear decay to 10% ✓ - - - ✓ ✓ ✓\\nGLaM (1.2T) 1M 1024 0.01 - inverse square root ✓ FP32 +✓ - ✓×\\nLaMDA (137B) 256K - - - - - - - - - - - - -\\nMT-NLG (530B) 1920 2048 5e-5 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nAlphaCode (41B) 2048 1536+768 1e-4 ✓ cosine decay to 10% ✓ ✓ ✓ ✓ -\\nChinchilla (70B) 1.5M 2048 1e-4 ✓ cosine decay to 10% ✓ ✓ - - -\\nPaLM (540B) 2048 2048 0.01 - inverse square root ✓ - - - ✓ ✓×\\nAlexaTM (20B) 2M 1024 1e-4 - linear decay to 5% ✓ ✓ ✓ - ✓\\nU-PaLM (540B) 32 2048 1e-4 - cosine ✓ - - - - - -\\nUL2 (20B) 1024 1024 - - inverse square root - - - - - -× - -\\nGLM (130B) 4224 2048 8e-5 ✓ cosine ✓ ✓ ✓ ✓ ✓\\nCodeGen (16B) 2M 2048 5e-5 ✓ cosine ✓ - - - ✓ ✓ -\\nLLaMA (65B) 4M Tokens 2048 1.5e-4 ✓ cosine decay to 10% ✓ - - - ✓ ✓ -\\nPanGu- Σ(1.085T) 512 1024 2e-5 ✓ - ✓ ✓ - - -\\nBloombergGPT (50B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓×\\nXuan Yuan 2.0 (176B) 2048 2048 6e-5 ✓ cosine ✓ ✓ ✓ ✓ -\\nCodeT5 +(16B) 2048 1024 2e-4 - linear ✓ ✓ ✓ - -\\nStarCoder (15.5B) 512 8k 3e-4 ✓ cosine ✓ ✓ ✓ - -\\nLLaMA-2 (70B) 4M Tokens 4k 1.5e-4 ✓ cosine ✓ ✓ ✓ ✓ -\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\\nno model uses weight decay for instruction tuning.\\nSequence Optimizers Grad\\nModels Batch Size Length LR Warmup LR_Decay AdaFactor Adam AdamW Clip Dropout\\nWebGPT (175B) BC:512, RM:32 -6e-5 - - ✓ - -\\nT0 (11B) 1024 1280 1e-3 - - ✓ - ✓\\nTk-Instruct (11B) 1024 -1e-5 - constant - - - - -\\nOPT-IML (175B) 128 2048 5e-5× linear ✓ ✓ ✓\\nFlan-U-PaLM (540B) 32 -1e-3 - constant ✓ - ✓\\nSparrow (70B) RM: 8 +16, RL:16 -2e-6 ✓ cosine decay to 10% ✓ ✓×\\nWizardCoder (15B) 512 2048 2e-5 ✓ cosine - - - - -\\nAlpaca (13B) 128 512 1e-5 ✓ cosine - - ✓ ✓×\\nVicuna (13B) 128 -2048 2e-5 ✓ cosine ✓ -×\\nLIMA (65B) 32 2048 1e-5× linear ✓ - ✓\\nQuAC [338]: This dataset simulates an information-seeking\\ndialog between students and teachers using hidden Wikipedia\\ntext. It introduces unique challenges not found in machine com-\\nprehension datasets, making it a valuable resource for advanc-\\ning dialog systems.\\n5.2.6. Commonsense Reasoning\\nHellaSwag [345]: A dataset that challenges models to pick the\\nbest ending to a context uses Adversarial Filtering to create a\\n‘Goldilocks’ zone of complexity, where generated text is absurd\\nto humans but often misclassified by models.\\nCOPA [391]: This dataset evaluates a model’s progress inopen-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability to\\nunderstand and reason about cause and e ffect.\\nWSC [347]: The Winograd Schema Challenge (WSC) is a\\nreading comprehension task in which a system must resolve\\nreferences in a text, often requiring world knowledge and rea-\\nsoning about the text.\\nCSQA [348]: The CommonsenseQA is a question-answering\\ndataset that requires commonsense knowledge to evaluate the\\nability of AI models to understand and answer questions.\\n27Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\nDataset Type Size/Samples Tasks Source Creation Comments\\nC4 [10] Pretrain 806GB - Common Crawl Automated A clean, multilingual dataset with billions\\nof tokens\\nmC4 [11] Pretrain 38.49TB - Common Crawl Automated A multilingual extension of the C4\\ndataset, mC4 identifies over 100 lan-\\nguages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [291] Pretrain 825GB -Common Crawl, PubMed Central,\\nOpenWebText2, ArXiv, GitHub,\\nBooks3, and othersAutomated A massive dataset comprised of 22 con-\\nstituent sub-datasets\\nROOTs [292] Pretrain 1.61TB - 498 Hugging Face datasets Automated 46 natural and 13 programming lan-\\nguages\\nMassiveText [116] Pretrain 10.5TB -MassiveWeb, Books, News,\\nWikipedia, Github, C4Automated 99% of the data is in English\\nWikipedia [293] Pretrain - - Wikipedia Automated Dump of wikipedia\\nRedPajama [294] Pretrain 5TB -CommonCrawl, C4, Wikipedia,\\nGithub, Books, StackExchangeAutomated Open-source replica of LLaMA dataset\\nPushShift.io Reddit Pretrain 21.1GB - Reddit Automated Submissions and comments on Reddit\\nfrom 2005 to 2019\\nBigPython [130] Pretrain 5.5TB Coding GitHub Automated -\\nPool of Prompt (P3) [17] Instructions 12M 62 PromptSource Manual A Subset of PromptSource, created from\\n177 datasets including summarization,\\nQA, classification, etc.\\nxP3 [144] Instructions 81M 71 P3+Multilingual datasets Manual Extending P3 to total 46 languages\\nSuper-NaturalInstructions (SNI) [18] Instructions 12.4M 1616 Multiple datasets Manual Extending P3 with additional multi-\\nlingual datasets, total 46 languages\\nFlan [16] Instructions 15M 1836 Muffin+T0-SF +NIV2 Manual Total 60 languages\\nOPT-IML [97] Instructions 18.1M 1667 - Manual -\\nSelf-Instruct [19] Instructions 82k 175 - Automated Generated 52k instructions with 82k sam-\\nples from 175 seed tasks using GPT-3\\nAlpaca [148] Instructions 52k - - Automated Employed self-instruct method to gener-\\nate data from text-davinci-003\\nVicuna [149] Instructions 125k - ShareGPT Automated Conversations shared by users on\\nShareGPT using public APIs\\nLLaMA-GPT-4 [150] Instructions 52k - Alpaca Automated Recreated Alpaca dataset with GPT-4 in\\nEnglish and Chinese\\nUnnatural Instructions [295] Instructions 68k - 15-Seeds (SNI) Automated -\\nLIMA [175] Instructions 1k - Multiple datasets Manual Carefully created samples to test perfor-\\nmance with fine-tuning on less data\\nAnthropic-HH-RLHF [296] Alignment 142k - - Manual\\nAnthropic-HH-RLHF-2 [168] Alignment 39k - - Manual\\n5.2.7. Reading Comprehension\\nBoolQ [353]: A dataset derived from Google search queries,\\nBoolQ challenges models to answer binary (yes /no) questions.\\nThe questions are naturally occurring and are paired with a\\nparagraph from a Wikipedia article containing the answer. It\\nis a test of reading comprehension and reasoning.\\nSQUADv2 [354]: The Stanford Question Answering Dataset\\n(SQuAD) [352] is a collection of questions posed by crowd\\nworkers on a set of Wikipedia articles, where the answer to ev-\\nery question is a segment of text from the corresponding reading\\npassage. SQuADv2 combines the original SQuAD1.1 dataset\\nwith over 50,000 unanswerable questions. The aim is to evalu-\\nate a model’s ability to understand and answer questions based\\non a given context and to determine when a question is unan-\\nswerable.\\nDROP [355]: DROP, or Discrete Reasoning Over the con-\\ntent of Paragraphs, is designed to test a model’s ability to un-\\nderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [356]: The Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textualentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\nWebQA [357]: A dataset for open-domain question answering,\\nWebQA o ffers a large collection of web-based question-answer\\npairs. It is designed to assess the ability of AI models to under-\\nstand and answer questions based on web content.\\nCMRC2018 [359]: This dataset is a test of Chinese language\\nmodels’ ability to reason comprehensively and is designed with\\na challenging span-extraction format that pushes the boundaries\\nof machine performance.\\n5.2.8. Mathematical Reasoning\\nMATH [372]: This dataset is a platform for evaluating the\\nmathematical problem-solving abilities of AI models. It con-\\ntains a diverse set of math problems, ranging from arithmetic\\nto calculus, and is designed to test the model’s ability to under-\\nstand and solve complex mathematical problems.\\nMath23k [373]: This one challenges a model’s ability to un-\\nderstand and solve mathematical word problems. It contains\\n23,000 Chinese arithmetic word problems that require models\\nto perform reasoning and computation based on the problem\\n28Table 9: Categorized evaluation datasets used in evaluating LLMs.\\nType Datasets /Benchmarks\\nMulti-Task MMLU [297], SuperGLUE [2], BIG-bench [298], GLUE [299], BBH [298], CUGE [300], Zero-\\nCLUE [301], FewCLUE [302], Blended Skill Talk [303], HELM [304], KLUE-STS [305]\\nLanguage Understanding CoQA [306], WiC [307], Wikitext103 [308], PG19 [309], LCQMC [310], QQP [311], WinoGender [312],\\nCB [313], FinRE [314], SanWen [315], AFQMC [301], BQ Corpus [316], CNSS [317], CKBQA 13 [318],\\nCLUENER [301], Weibo [319], AQuA [320], OntoNotes [321], HeadQA [322], Twitter Dataset [323]\\nStory Cloze and\\nSentence CompletionStoryCloze [324], LAMBADA [325], LCSTS [326], AdGen [327], E2E [328], CHID [329], CHID-\\nFC [302]\\nPhysical Knowledge and\\nWorld UnderstandingPIQA [330], TriviaQA [331], ARC [332], ARC-Easy [332], ARC-Challenge [332], PROST [333], Open-\\nBookQA [334], WebNLG [335], DogWhistle Insider & Outsider [336]\\nContextual Language\\nUnderstandingRACE [337], RACE-Middle [337], RACE-High [337], QuAC [338], StrategyQA [339], Quiz Bowl [340],\\ncMedQA [341],cMedQA2 [342], MATINF-QA [343]\\nCommonsense Reasoning WinoGrande [344], HellaSwag [345], COPA [346], WSC [347], CSQA [348], SIQA [349], C3[350],\\nCLUEWSC2020 [301], CLUEWSC [301], CLUEWSC-FC [302], ReCoRD [351]\\nReading Comprehension SQuAD [352], BoolQ [353], SQUADv2 [354], DROP [355], RTE [356], WebQA [357], CMRC2017 [358],\\nCMRC2018 [359], CMRC2019 [360], COTE-BD [361], COTE-DP [361], COTE-MFW [361], Mul-\\ntiRC [362], Natural Questions [363], CNSE [317], DRCD [364], DuReader [365], Dureader robust [366],\\nDuReader-QG [365], SciQ [367], Sogou-log [368], Dureader robust-QG [366], QA4MRE [369], KorQuAD\\n1.0 [370], CAIL2018-Task1 & Task2 [371]\\nMathematical Reasoning MATH [372], Math23k [373], GSM8K [374], MathQA [375], MGSM [376], MultiArith [377], AS-\\nDiv [378], MAWPS [379], SV AMP [380]\\nProblem Solving HumanEval [131], DS-1000 [381], MBPP [382], APPS [372], CodeContests [132]\\nNatural Language Inference\\n& Logical ReasoningANLI [383], MNLI-m [384], MNLI-mm [384],QNLI [352], WNLI [347], OCNLI [301], CMNLI [301],\\nANLI R1 [383], ANLI R2 [383], ANLI R3 [383], HANS [385], OCNLI-FC [302], LogiQA [386], Strate-\\ngyQA [339]\\nCross-Lingual Understanding MLQA [387], XNLI [388], PAWS-X [389], XSum [390], XCOPA [391], XWinograd [392], TyDiQA-\\nGoldP [393], MLSum [394]\\nTruthfulness and Fact Checking TruthfulQA [395], MultiFC [396], Fact Checking on Fever [397]\\nBiases and Ethics in AI ETHOS [398], StereoSet [399], BBQ [400], Winobias [401], CrowS-Pairs [402]\\nToxicity RealToxicityPrompts [403], CivilComments toxicity classification [404]\\nLanguage Translation WMT [405], WMT20 [406], WMT20-enzh [406], EPRSTMT [302], CCPM [407]\\nScientific Knowledge AminoProbe [138], BioLAMA [138], Chemical Reactions [138], Galaxy Clusters [138], Mineral\\nGroups [138]\\nDialogue Wizard of Wikipedia [408], Empathetic Dialogues [409], DPC-generated [96] dialogues, ConvAI2 [410],\\nKdConv [411]\\nTopic Classification TNEWS-FC [302], YNAT [305], KLUE-TC [305], CSL [301], CSL-FC [302], IFLYTEK [412]\\ndescription.\\nGSM8K [374]: A dataset of diverse grade school math word\\nproblems, testing a model’s ability to perform multi-step math-\\nematical reasoning.\\n5.2.9. Problem Solving and Logical Reasoning\\nANLI [383]: A large-scale dataset designed to test the robust-\\nness of machine learning models in Natural Language Inference\\n(NLI) is created through an iterative, adversarial process where\\nhumans try to generate examples that models cannot correctly\\nclassify.\\nHumanEval [131]: A dataset for evaluating the problem-\\nsolving ability of AI models, which includes a diverse set of\\ntasks that require various cognitive abilities, making it a com-\\nprehensive tool for assessing general intelligence in AI.\\nStrategyQA [339]: A question-answering dataset that re-\\nquires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the bound-\\naries of what machines can understand and answer.5.2.10. Cross-Lingual Understanding\\nXNLI [388]: A cross-lingual benchmark, XNLI extends the\\nMultiNLI [419] corpus to 15 languages, including low-resource\\nones like Urdu. It tests models on cross-lingual sentence under-\\nstanding, with 112,500 annotated pairs across three categories:\\nentailment, contradiction, and neutral.\\nPAWS-X [389]: PAWS-X, or Cross-lingual Paraphrase Adver-\\nsaries from Word Scrambling, is a multilingual version of the\\nPAWS [420] dataset for paraphrase identification. It includes\\nexamples in seven languages and is designed to evaluate the\\nperformance of cross-lingual paraphrase identification models.\\n5.2.11. Truthfulness\\nTruthful-QA [395]: A unique benchmark that measures a\\nlanguage model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some designed to test the model against com-\\nmon human misconceptions.\\n29Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\\nis natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\\n“Mem.” is memorization.\\nBenchmark\\nModels Training DatasetBIG-\\nbenchMMLUSuper\\nGLUEQA Clf NLI MTCloze /\\nCompletionRC CR MR CodingTruthful /\\nBias /\\nToxicity /\\nMem.\\nT5 C4 [10] ✓ ✓ ✓✓ ✓ ✓✓✓\\nGPT-3 Common Crawl, WebText, Books Cor-\\npora, Wikipedia✓ ✓ ✓ ✓ ✓ ✓\\nmT5 mC4 [11] ✓ ✓✓\\nPanGu-α 1.1TB Chinese Text Corpus ✓ ✓ ✓ ✓✓\\nCPM-2 WuDaoCorpus [109] ✓ ✓\\nCodex 54 million public repositories from Github ✓\\nERNIE-3.0 Chinese text corpora, Baidu Search, Web\\ntext, QA-long, QA-short, Poetry and Cou-\\nplet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge\\ngraph with more than 50 million facts✓ ✓✓✓✓ ✓ ✓ ✓\\nJurassic-1 Wikipedia, OWT, Books, C4, Pile [291],\\narXiv, GitHub✓ ✓ ✓ ✓\\nHyperCLOV A Korean blogs, Community sites, News,\\nKiN Korean Wikipedia, Wikipedia (En-\\nglish and Japanese), Modu-Corpus: Mes-\\nsenger, News, Spoken and written lan-\\nguage corpus, Web corpus✓\\nYuan 1.0 Common Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books✓✓✓ ✓\\nGopher subsets of MassiveWeb Books, C4, News,\\nGitHub and Wikipedia samples from Mas-\\nsiveText✓ ✓ ✓ ✓ ✓✓ ✓\\nERNIE-3.0 TITAN Same as ERNIE 3.0 and ERNIE 3.0 ad-\\nversarial dataset, ERNIE 3.0 controllable\\ndataset✓✓✓ ✓ ✓\\nGPT-NeoX-20B Pile [291] ✓ ✓ ✓ ✓ ✓✓\\nOPT RoBERTa [289], Pile [291], PushShift.io\\nReddit [413]✓✓ ✓ ✓\\nBLOOM ROOTs [13] ✓ ✓✓ ✓ ✓ ✓\\nGalactica arXiv, PMC, Semantic Scholar, Wikipedia,\\nStackExchange, LibreText, Open Text-\\nbooks, RefSeq Genome, OEIS, LIPID\\nMAPS, NASAExoplanet, Common Crawl,\\nScientificCC, AcademicCC, GitHub repos-\\nitories Khan Problems, GSM8K, OneS-\\nmallStep✓ ✓ ✓ ✓ ✓\\nGLaM Filtered Webpages, Social media conversa-\\ntions Wikipedia, Forums, Books, News✓ ✓ ✓ ✓✓\\nLaMDA Infiniset : Public documents, Dialogs, Ut-\\nterances✓\\nMT-NLG Two snapshots of Common Crawl and\\nBooks3, OpenWebText2, Stack Exchange,\\nPubMed Abstracts, Wikipedia, PG-19\\n[242], BookCorpus2, NIH ExPorter, Pile,\\nCC-Stories, RealNews✓ ✓ ✓✓ ✓\\nAlphaCode Selected GitHub repositories, CodeCon-\\ntests: Codeforces, Description2Code, Co-\\ndeNet✓\\nChinchilla MassiveWeb, MassiveText Books, C4,\\nNews, GitHub, Wikipedia✓ ✓ ✓ ✓✓ ✓\\nPaLM webpages, books, Wikipedia, news, arti-\\ncles, source code, social media conversa-\\ntions✓ ✓ ✓ ✓ ✓ ✓\\nAlexaTM Wikipedia, mC4 ✓ ✓✓ ✓ ✓\\nU-PaLM Same as PaLM ✓ ✓ ✓ ✓ ✓ ✓✓\\nUL2 - ✓ ✓✓✓ ✓ ✓\\nGLM-130B - ✓ ✓ ✓\\nCodeGen Pile, BigQuery, BigPython ✓\\nLLaMA CommonCrawl, C4, Github, Wikipedia,\\nBooks, arXiv, StackExchange✓ ✓ ✓✓✓ ✓ ✓\\nPanGu- Σ WuDaoCorpora, CLUE, Pile, C4, Python\\ncode✓✓✓✓ ✓ ✓\\nBloombergGPT inPile, Pile, C4, Wikipedia ✓ ✓ ✓ ✓ ✓✓ ✓\\nCodeT5 + CodeSearchNet, Github Code ✓ ✓\\nStarCoder The Stack v1.2 ✓ ✓ ✓ ✓\\nLLaMA-2 ✓ ✓ ✓ ✓✓✓ ✓\\nPaLM-2 Web documents, Code, Books, Maths,\\nConversation✓ ✓✓✓✓ ✓ ✓✓✓ ✓ ✓\\n30Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\nModels Training DatasetBIG-\\nbenchMMLU BBH RAFT FLAN SNI PromptSource TyDiQA HumanEval MBPPTruthful /\\nBias /\\nToxicity\\nT0 Pool of Prompts ✓\\nWebGPT ELI5 [414], ELI5 fact-\\ncheck [156], TriviaQA [331],\\nARC-Challenge [332], ARC-\\nEasy [332], Hand-written data,\\nDemonstrations of humans, Com-\\nparisons between model-generated\\nanswers✓\\nTk-INSTRUCT SNI [18] ✓\\nmT0 xP3 [144]\\nOPT-IML PromptSource [17], FLAN [16],\\nSNI [415], UnifiedSKG [416],\\nCrossFit [417], ExMix [418],\\nT5 [10], Reasoning✓ ✓ ✓ ✓ ✓ ✓\\nFlan Muffin, T0-SF, NIv2, CoT ✓ ✓ ✓\\nWizardCoder Code Alpaca ✓ ✓\\n5.2.12. Biases and Ethics in AI\\nETHOS [398]: ETHOS is a hate speech detection dataset\\nbuilt from YouTube and Reddit comments. It is a tool in the\\nfight against online hate speech, o ffering binary and multi-label\\nvariants for robust content moderation.\\nStereoSet [399]: StereoSet is a comprehensive dataset de-\\nsigned to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. Contrasting stereotypi-\\ncal bias against language modeling ability provides a valuable\\ntool for understanding and mitigating biases in large language\\nmodels.\\n6. Applications\\nApplying Large Language Models (LLMs) to a variety of\\ndownstream tasks has become a popular trend in both AI-\\nrelated research communities and industries, with many emerg-\\ning uses being discovered and explored daily. LLMs, which are\\ncapable of understanding and generating human-like text, have\\nfound meaningful applications across a variety of fields. This\\nsection provides an overview of LLM applications in medicine,\\neducation, science, mathematics, law, finance, robotics, and\\ncoding. While each of these domains pose di fferent challenges,\\nLLMs open up opportunities to make significant contributions\\nto these domains through their generalizability.\\nGeneral Purpose: LLMs are being widely considered as\\ngeneral-purpose tools for a wide variety of tasks [421]. This\\nis due to their inherent ability to understand, generate, and\\nmanipulate human-like text in a contextually relevant man-\\nner. This allows them to perform tasks ranging from simple\\nlanguage translation and question-answering to more complex\\ntasks like summarization, text generation, and even program-\\nming help [422]. The utility of LLMs is further enhanced by\\ntheir ability to adapt to the specific style and tone of the text\\nthey are processing, making the outputs more user-friendly and\\ncontext-aware. In everyday applications, LLMs can be used as\\npersonal assistants, helping users draft emails or schedule ap-\\npointments [423]; they can also be deployed in customer ser-vice to handle common questions; or applied to generate con-\\ntent for digital platforms like websites, by creating human-like\\ntext based on given prompts [424]. Moreover, LLMs play a cru-\\ncial role in data analysis, where they can filter large volumes of\\ntext data, summarize key points, and find patterns that would\\ntake humans much longer to identify [425]. Despite their wide-\\nranging applications, it is essential to remember that LLMs,\\nsimilar to any AI system, are only as good as the data they have\\nbeen trained on.\\nMedicine: The application of LLMs in the field of medicine is\\nreshaping healthcare delivery and research. For example, LLMs\\nare increasingly used in clinical decision support systems to\\nprovide physicians with evidence-based treatment recommen-\\ndations [426, 427, 428]. By analyzing patient data and medical\\nliterature, they can help identify potential diagnoses, suggest\\nappropriate tests, and recommend optimal treatment strategies.\\nMoreover, LLMs can also enhance patient interactions with\\nhealthcare systems; e.g., they can be used in chatbot applica-\\ntions [429, 430, 431] to answer patient queries about symptoms\\nor medications, schedule appointments, and even provide es-\\nsential health advice. For medical research, LLMs are used to\\nextract and filter information from a considerable amount of\\nmedical literature, identify relevant studies, summarize find-\\nings, and even predict future research trends [432, 433, 434].\\nFor medical education, LLMs can help create training mate-\\nrials, generate exam questions, provide detailed explanations\\nof complex medical topics, and o ffer personalized feedback to\\nstudents [435, 436, 437, 438]. They can also simulate patient\\ninteractions, enabling students to practice and improve their\\nclinical skills. At a broader level, LLMs can assist in public\\nhealth initiatives by analyzing media data to detect disease out-\\nbreaks, monitor public sentiment towards health policies, and\\ndisseminate health information in a clear and understandable\\nmanner [439]. LLMs can be employed to support public health\\ninitiatives, addressing related issues such as data privacy, the\\nnecessity for explainability, and the potential risk of propagat-\\ning biases [440, 441].\\nEducation: The integration of LLMs into the educational sec-\\ntor offers opportunities to enhance learning experiences, teacher\\n31Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\\nto the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\\nbenchmark.\\nTask Dataset /BenchmarkTop-1 Top-2 Top-3\\nModel (Size) Score (N-shots) Model (Size) Score (N-shots) Model (Size) Score (N-shots)\\nMulti-TaskBIG-bench (B) Chinchilla (70B) 65.1 (5-shot) Gopher (280B) 53.97 (5-shot) PaLM (540B) 53.7 (5-shot)\\nMMLU (B) GPT-4 (-) 86.4 (5-shot) Gemini (Ultra) 83.7 (5-shot) Flan-PaLM-2 (f)(Large) 81.2 (5-shot)\\nLanguage Understanding SuperGLUE (B) ERNIE 3.0 (12B) 90.6 (-) PaLM (f)(540B) 90.4 (-) T5 (11B) 88.9 (-)\\nStory Comprehension and\\nGenerationHellaSwag GPT-4 (-) 95.3 (10-shot) Gemini (Ultra) 87.8 (10-shot) PaLM-2 (Large) 86.8 (one shot)\\nStoryCloze GPT3 (175B) 87.7 (few shot) PaLM-2 (Large) 87.4 (one shot) OPT (175B) 79.82 (-)\\nPhysical Knowledge and\\nWorld UnderstandingPIQA PaLM-2 (Large) 85.0 (one shot) LLaMa (65B) 82.8 (zero shot) MT-NLG (530B) 81.99 (zero shot)\\nTriviaQA PaLM-2 (Large) 86.1 (one shot) LLaMA-2 (70B) 85.0 (one shot) PaLM (540B) 81.4 (one shot)\\nContextual Language\\nUnderstandingLAMBADA PaLM (540B) 89.7 (few shot) MT-NLG (530B) 87.15 (few shot) PaLM-2 (Large) 86.9 (one shot)\\nCommonsense ReasoningWinoGrande GPT-4 (-) 87.5 (5-shot) PaLM-2 (Large) 83.0 (one shot) PaLM (540B) 81.1 (zero shot)\\nSIQA LLaMA (65B) 52.3 (zero shot) Chinchilla (70B) 51.3 (zero shot) Gopher (280B) 50.6 (zero shot)\\nReading Comprehension BoolQ PaLM (f)(540B) 92.2 (-) T5 (11B) 91.2 (-) PaLM-2 (Large) 90.9 (one shot)\\nTruthfulness Truthful-QA LLaMA (65B) 57 (-)\\nMathematical ReasoningMATH Gemini (Ultra) 53.2 (4-shot) PaLM-2 (Large) 34.3 (4-shot) LLaMa-2 (65B) 13.5 (4-shot)\\nGSM8K GPT-4 (-) 92.0 (5-shot) PaLM-2 (Large) 80.7 (8-shot) U-PaLM (540B) 58.5 (-)\\nProblem Solving and\\nLogical ReasoningHumanEval Gemini (f)(Ultra) 74.4 (zero shot) GPT-4 (-) 67.0 (zero shot) Code Llama (34B) 48.8 (zero shot)\\nsupport, and educational content development. For students, by\\nanalyzing their learning styles, performance, and preferences,\\nLLMs can provide customized study materials and practice\\nquestions to develop personalized learning experiences [442].\\nFor teachers, LLMs can help to create lesson plans and grade\\nassignments and generate diverse and inclusive educational\\ncontent, significantly saving more time for teaching and student\\ninteraction [443, 444]. In language learning, LLMs serve as\\nadvanced conversational partners capable of simulating conver-\\nsations in multiple languages, correcting grammar, enhancing\\nvocabulary, and aiding pronunciation for the needs of fluency\\nin practice [445]. Furthermore, LLMs improve accessibility\\nin education by providing support for students with disabili-\\nties. They can generate real-time transcriptions for the hear-\\ning impaired, o ffer reading assistance for the visually impaired,\\nand simplify complex texts for those with learning disabili-\\nties [441]. As LLMs continue to evolve, their applications in\\neducation can benefit more students and teachers from di fferent\\nperspectives in practice.\\nScience: Similar to medical applications, LLMs can expedite\\nthe research process by quickly analyzing and summarizing sci-\\nentific literature. By briefing comprehensible and accessible re-\\nsearch summaries, LLMs can assist researchers in staying up-\\nto-date with the latest findings, even in fields outside their area\\nof expertise [446, 447]. In addition, LLMs can aid scientists\\nin formulating new hypotheses and research questions since\\ntheir ability to process large-scale datasets allows them to un-\\nveil insights that might not be immediately apparent to human\\nresearchers [448]. Moreover, for scientific writing, LLMs can\\nhelp researchers draft documents, suggest improvements, and\\nensure adherence to specific formatting guidelines [449, 450].\\nThis not only saves time but also improves the clarity of scien-\\ntific communication, enabling interdisciplinary teams to work\\ntogether more e ffectively.\\nMaths: In addition to providing mathematical research and\\neducation support, LLMs can assist in solving mathematical\\nproblems by giving step-by-step explanations and guiding users\\nthrough complex proofs and calculations. They can help iden-tify errors in reasoning or computation and suggest corrections,\\nserving as an invaluable tool for both learning and verification\\npurposes [451, 452]. LLMs can be employed to check the valid-\\nity of mathematical proofs, o ffering a preliminary filter before\\nhuman review. While they are not a substitute for the meticu-\\nlous work of mathematicians, they can help simplify the process\\nof proof verification [453, 454]. Moreover, LLMs enhance ac-\\ncessibility to mathematics by translating complex concepts and\\nfindings into understandable language for non-specialists [455],\\nwhere the gap between theoretical mathematics and applied\\ncontexts such as physics, engineering, and economics can be\\nbridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-\\numents, including generating initial coding for datasets, iden-\\ntifying themes, and classifying data according to these themes.\\nThis collaborative e ffort between legal experts and LLMs has\\nproved to be e ffective in analyzing legal texts such as court\\nopinions on theft, improving both the e fficiency and quality of\\nthe research [456]. Additionally, LLMs have been evaluated for\\ntheir ability to generate explanations of legal terms, focusing\\non improving factual accuracy and relevance by incorporating\\nsentences from case law. By feeding relevant case law into the\\nLLM, the augmented models can generate higher-quality expla-\\nnations with less factually incorrect information [457]. More-\\nover, LLMs can be trained with specialized domain knowledge\\nto perform legal reasoning tasks [458] and answer legal ques-\\ntions [459].\\nFinance: LLMs like BloombergGPT [141], trained on exten-\\nsive proprietary financial datasets, exhibit superior performance\\non financial tasks. This indicates the value of domain-specific\\ntraining in creating LLMs that can more accurately understand\\nand process industry-specific language and concepts. The intro-\\nduction of FinGPT [460] as an open-source model o ffers trans-\\nparent and accessible resources to develop novel applications\\nsuch as robo-advising, algorithmic trading, and low-code so-\\nlutions, ultimately expanding the capabilities of financial ser-\\nvices. Both BloombergGPT and FinGPT show the adaptabil-\\nity of LLMs to the financial domain, with the former showing\\n32the power of custom datasets and the latter emphasizing a data-\\ncentric approach and low-rank adaptation techniques for cus-\\ntomization. Moreover, LLMs demonstrate an ability to break\\ndown complex financial tasks into actionable plans, enabling\\nend-to-end solutions that were previously unfeasible with a sin-\\ngle model [461].\\nRobotics: In robotics research, LLMs have promising appli-\\ncations, such as enhancing human-robot interaction [28, 462,\\n463, 464], task planning [227], motion planning [236], nav-\\nigation [236, 465], object manipulation [226], personalized\\nrobots [466], etc. LLMs enable robots to understand the en-\\nvironment e ffectively and generate plans to complete tasks col-\\nlaboratively [230, 26]. They can facilitate continuous learning\\nby allowing robots to access and integrate information from a\\nwide range of sources, helping robots acquire new skills, adapt\\nto changes, and refine their paths [214, 223, 224].\\n7. Challenges and Future Directions\\nLLMs such as GPT-4 and its predecessors have significantly\\nadvanced natural language processing. Nevertheless, they also\\nbring along a set of challenges. The computational cost, ad-\\nversarial robustness, and interpretability are among the tech-\\nnical challenges that are intrinsic to these models. Further-\\nmore, as these models are scaled up to handle more complex\\ntasks or to operate in more complex or dynamic environments,\\nnew challenges in scalability, privacy, and real-time processing\\nemerge. On the frontier of foundational research, integrating\\nmulti-modality and the e ffectiveness of transfer learning are be-\\ning keenly explored. Additionally, the continuous learning as-\\npect of these models, which aims to have models that can adapt\\nto new information over time, presents a fresh set of challenges.\\nThese challenges not only underscore the technical intricacies\\ninvolved but also highlight the broader impact and the future\\ntrajectory of LLMs in real-world applications. The following\\nsections delve into these challenges, shedding light on the on-\\ngoing and potential e fforts to address them.\\nComputational Cost: Training LLMs requires extensive com-\\nputational resources, which increases production costs and\\nraises environmental concerns due to substantial energy con-\\nsumption during large-scale training. Improved performance\\noccurs as computational resources increase, but the rate of\\nimprovement gradually decreases when both the model and\\ndataset size remain fixed, following the power law of dimin-\\nishing returns [467].\\nBias and Fairness: LLMs can inherit and amplify societal bi-\\nases in their training data. These biases can manifest in the\\nmodel’s outputs, leading to potential ethical and fairness is-\\nsues [468].\\nOverfitting: Although LLMs possess substantial learning ca-\\npabilities, they are susceptible to overfitting noisy and peculiar\\npatterns within their extensive training data. Consequently, this\\nmay cause them to generate illogical responses [469]. The de-\\nbate about Memorization vs. Generalization in LLMs is about\\nfinding the right balance. Memorization allows the model to\\nremember specific details from its training data, ensuring it canprovide accurate answers to precise questions. However, gen-\\neralization enables the model to make inferences and produce\\nresponses for inputs it has not seen before, which is essential\\nfor handling various real-world tasks. Striking the right bal-\\nance is the challenge: too much memorization can lead to over-\\nfitting, making the model inflexible and struggling with new\\ninputs [470].\\nEconomic and Research Inequality: The high cost of train-\\ning and deploying LLMs may make their development concen-\\ntrated within well-funded organizations, potentially worsening\\neconomic and research inequalities in AI [471].\\nReasoning and Planning: Some reasoning and planning tasks,\\neven as seemingly simple as common-sense planning, which\\nhumans find easy, remain well beyond the current capabilities\\nof LLMs evaluated using an assessment framework. This is not\\nentirely unexpected, considering that LLMs primarily generate\\ntext completions based on likelihood and o ffer no solid guaran-\\ntees in terms of reasoning abilities [472].\\nHallucinations: LLMs exhibit “hallucinations\", where they\\ngenerate responses that, while sounding plausible, are incor-\\nrect or do not align with the provided information [473]. The\\nhallucination can be categorized into three categories.\\n•Input-conflicting hallucination, wherein LLMs produce\\ncontent that diverges from the input given by users.\\n•Context-conflicting hallucination, where LLMs generate\\ncontent that contradicts information they have generated\\nearlier.\\n•Fact-conflicting hallucination involves LLM’s generation\\nof content that does not align with established world\\nknowledge.\\nPrompt Engineering: Prompts serve as inputs to LLMs, and\\ntheir syntax and semantics play a crucial role in determining\\nthe model’s output. The prompt variations, sometimes counter-\\nintuitive to humans, can result in significant changes in model\\noutput and are addressed through prompt engineering, which\\ninvolves designing natural language queries to guide LLMs\\nresponses e ffectively [474, 32].\\nLimited Knowledge: Information acquired during pretraining\\nis limited and may become obsolete after some time. Re-\\ntraining the model using updated data is costly. To generate\\nfactually accurate responses people use a retrieval augmen-\\ntation pipeline [188]. However, pre-trained models are not\\ntrained with retrieval augmentation generation (RAG) [6, 21],\\nhence, adapting the training pipeline is necessary [183, 25].\\nSafety and Controllability: Using LLMs comes with the risk\\nof generating harmful, misleading, or inappropriate content,\\nwhether by accident or when given specific prompts. Ensuring\\nthese models are safely utilized is a significant concern [475].\\nMulti-Modality: Multi-modal learning, where LLMs are\\ntrained on diverse data like text, images, and videos, aims to\\ncreate models with richer understanding but faces challenges\\nin data alignment, fusion strategies, and higher computational\\ndemands.\\nCatastrophic Forgetting: LLMs are often pre-trained on large\\n33datasets and then fine-tuned on domain-specific data, reducing\\ntraining resources but facing issues like domain adaptation and\\ncatastrophic forgetting, which hinders the retention of original\\nknowledge when learning new tasks.\\nAdversarial Robustness: Large Language Models (LLMs)\\nhave shown great capabilities in various tasks but are vul-\\nnerable to adversarial attacks, where slight, deliberate input\\nalterations can mislead them. Especially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-\\nthough it sometimes compromises generalization [476]. As\\nLLMs integrate more into complex systems, examining their\\nsecurity properties becomes crucial, given the emerging field\\nof adversarial attacks on LLMs within trustworthy ML [477].\\nThis vulnerability is notable in safety-critical domains, ne-\\ncessitating robust adversarial evaluation tools to ensure LLM\\nreliability [478].\\nInterpretability and Explainability: The \"black-box\" nature\\nof LLMs poses challenges in understanding their decision-\\nmaking, which is crucial for broader acceptance and trust,\\nespecially in sensitive domains. Despite their advanced\\ncapabilities, the lack of insight into their operation limits their\\neffectiveness and trustworthiness [479, 480]. E fforts are being\\nmade to make LLMs more explainable to promote user trust\\nand to ensure responsible AI usage. Understanding the logic\\nbehind LLMs’ responses is essential for fostering trust and\\nensuring they align with human values and legal standards.\\nPrivacy Concerns: Privacy concerns in Large Language\\nModels (LLMs) have escalated with their growth in complexity\\nand size, particularly around data sharing and potential misuse.\\nThere is a risk of malicious content creation, filter bypass,\\nand data privacy issues, especially in e-commerce, where\\nprotecting customer privacy is crucial. If models are trained\\non private data, additional concerns arise if such models are\\nmade publicly available. LLMs tend to memorize phrases from\\ntheir training sets, which an adversary could exploit to extract\\nsensitive data, posing a threat to personal privacy [481, 482].\\nReal-Time Processing: Real-time processing in Large Lan-\\nguage Models (LLMs) is pivotal for various applications,\\nespecially with the rising popularity of mobile AI applications\\nand concerns regarding information security and privacy.\\nHowever, LLMs often have hundreds of layers and millions\\nof parameters, which impede real-time processing due to the\\nhigh computational demands and limited weight storage on\\nhardware platforms, particularly in edge computing environ-\\nments [483]. While certain e fforts like MobileBERT aim\\nto reduce memory requirements, they still face substantial\\nexecution overhead due to the large number of model layers,\\nleading to high inference latency.\\nLong-Term Dependencies: Large Language Models (LLMs)\\nhave shown considerable progress in understanding and\\ngenerating text, yet they often struggle with preserving context\\nand handling long-term dependencies, particularly in complex,\\nmulti-turn conversations or long documents. This limitation\\ncan lead to incoherent or irrelevant responses.\\nHardware Acceleration: The growth of LLMs presents signif-\\nicant hardware challenges due to the increasing computational\\nand memory demands associated with training and deployingthese models. GPUs have played a crucial role in meeting the\\nhardware requirements for training LLMs, with the networking\\nindustry also evolving to optimize hardware for training\\nworkloads. However, the growing size of LLMs, which has\\nbeen outpacing hardware progress, makes model inference in-\\ncreasingly costly. Model quantization is a promising approach\\nto bridge the widening gap between LLM size and hardware\\ncapacity [484]. Although specialized hardware acceleration\\nlike GPUs or TPUs can significantly reduce the computational\\ncost, making real-time applications more feasible, they may not\\nfully resolve all limitations, necessitating further advancements\\nin hardware technology.\\nRegulatory and Ethical Frameworks: The rapid advancements\\nin artificial intelligence have given rise to sophisticated Large\\nLanguage Models (LLMs) like OpenAI’s GPT-4 [147] and\\nGoogle’s Bard. These developments underscore the imperative\\nfor regulatory oversight to manage the ethical and social\\nchallenges accompanying LLMs’ widespread use [485]. For\\ninstance, LLMs can generate content that can be used posi-\\ntively or negatively, emphasizing the need for proactive ethical\\nframeworks and policy measures to guide their responsible\\nuse and assign accountability for their outputs [486]. Auditing\\nis identified as a promising governance mechanism to ensure\\nthat AI systems, including LLMs, are designed and deployed\\nethically, legally, and technically robust [487].\\n8. Conclusion\\nThis article has reviewed the developments on LLMs com-\\nprehensively. It contributes to summarizing significant find-\\nings of LLMs in the existing literature and provides a de-\\ntailed analysis of the design aspects, including architectures,\\ndatasets, and training pipelines. We identified crucial archi-\\ntectural components and training strategies employed by dif-\\nferent LLMs. These aspects are presented as summaries and\\ndiscussions throughout the article. Moreover, we have dis-\\ncussed the performance di fferences of LLMs in zero-shot and\\nfew-shot settings, explored the impact of fine-tuning, and com-\\npared supervised and generalized models and encoder vs. de-\\ncoder vs. encoder-decoder architectures. A comprehensive re-\\nview of multi-modal LLMs, retrieval augmented LLMs, LLMs-\\npowered agents, e fficient LLMs, datasets, evaluation, applica-\\ntions, and challenges is also provided. This article is anticipated\\nto serve as a valuable resource for researchers, o ffering insights\\ninto the recent advancements in LLMs and providing funda-\\nmental concepts and details to develop better LLMs.\\nReferences\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\\ntory” for natural language processing?, in: Machine Learning and\\nKnowledge Discovery in Databases. Research Track: European Con-\\nference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\\nProceedings, Part III 21, Springer, 2021, pp. 677–693. 1\\n[2] A. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, S. Bowman, Superglue: A stickier benchmark for general-\\npurpose language understanding systems, Advances in neural informa-\\ntion processing systems 32 (2019). 1, 24, 29\\n34[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\\nZ. Yang, A. Kulshreshtha, G. Nemade, Y . Lu, et al., Towards a human-\\nlike open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\\n[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n151 (2) (2022) 183–197. 2\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\\nLanguage models are unsupervised multitask learners, OpenAI blog\\n1 (8) (2019) 9. 2, 7\\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\\nare few-shot learners, Advances in neural information processing sys-\\ntems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 17, 22, 23, 24, 25, 33\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018). 2, 18, 24\\n[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nL. Zettlemoyer, Deep contextualized word representations, in: NAACL-\\nHLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\\n2\\n[9] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehen-\\nsion, arXiv preprint arXiv:1910.13461 (2019). 2\\n[10] C. Ra ffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\\na unified text-to-text transformer, The Journal of Machine Learning Re-\\nsearch 21 (1) (2020) 5485–5551. 2, 7, 8, 17, 19, 24, 25, 26, 28, 30,\\n31\\n[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, C. Ra ffel, mt5: A massively multilingual pre-trained text-to-\\ntext transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\\n25, 28, 30\\n[12] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y . Yao, F. Qi,\\nJ. Guan, P. Ke, et al., Cpm-2: Large-scale cost-e ffective pre-trained lan-\\nguage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\\nparameter open-access multilingual language model, arXiv preprint\\narXiv:2211.05100 (2022). 2, 4, 9, 11, 22, 23, 24, 25, 30\\n[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\\nM. Diab, X. Li, X. V . Lin, et al., Opt: Open pre-trained transformer\\nlanguage models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 23,\\n24, 25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\\ning language modeling with pathways, arXiv preprint arXiv:2204.02311\\n(2022). 2, 6, 9, 10, 22, 23, 24, 25\\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\\nlanguage models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 17,\\n22, 23, 25, 28, 31\\n[17] V . Sanh, A. Webson, C. Ra ffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Cha ffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\\nprompted training enables zero-shot task generalization, arXiv preprint\\narXiv:2110.08207 (2021). 2, 11, 25, 28, 31\\n[18] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\\nSuper-naturalinstructions: Generalization via declarative instructions on\\n1600+nlp tasks, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\\n11, 17, 23, 25, 28, 31\\n[19] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\\njishirzi, Self-instruct: Aligning language model with self generated in-\\nstructions, arXiv preprint arXiv:2212.10560 (2022). 2, 11, 18, 22, 28\\n[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\\nels to follow instructions with human feedback, Advances in Neural In-\\nformation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\\n22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Openfoundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\\n(2023). 2, 7, 10, 16, 25, 33\\n[22] J. Wei, Y . Tay, R. Bommasani, C. Ra ffel, B. Zoph, S. Borgeaud, D. Yo-\\ngatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\\nlarge language models, arXiv preprint arXiv:2206.07682 (2022). 2\\n[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\\nlanguage models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\\n[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\\nentific research capabilities of large language models, arXiv preprint\\narXiv:2304.05332 (2023). 2\\n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\\nretrieval augmented language models, arXiv preprint arXiv:2208.03299\\n(2022). 2, 17, 18, 33\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 19, 21, 33\\n[27] A. Parisi, Y . Zhao, N. Fiedel, Talm: Tool augmented language models,\\narXiv preprint arXiv:2205.12255 (2022). 2, 18, 19\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models\\nfor human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\\n33\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y . Zhou, J. Wang, A. Hu, P. Shi,\\nY . Shi, et al., mplug-owl: Modularization empowers large language\\nmodels with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\\n22\\n[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\\nT. Lu, J. Zhou, Y . Qiao, et al., Visionllm: Large language model\\nis also an open-ended decoder for vision-centric tasks, arXiv preprint\\narXiv:2305.11175 (2023). 2, 22\\n[31] R. Yang, L. Song, Y . Li, S. Zhao, Y . Ge, X. Li, Y . Shan, Gpt4tools:\\nTeaching large language model to use tools via self-instruction, arXiv\\npreprint arXiv:2305.18752 (2023). 2, 19, 22\\n[32] E. Saravia, Prompt Engineering Guide, https: //github.com /dair-\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 17, 33\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\\nmodel, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 22, 23, 25\\n[34] Y . Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5 +:\\nOpen code large language models for code understanding and genera-\\ntion, arXiv preprint arXiv:2305.07922 (2023). 2, 10, 24, 25\\n[35] S. Wang, Y . Sun, Y . Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\\nY . Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\\nedge enhanced pre-training for language understanding and generation,\\narXiv preprint arXiv:2112.12731 (2021). 2, 8, 23, 25\\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y . He, Deepspeed: System op-\\ntimizations enable training deep learning models with over 100 billion\\nparameters, in: Proceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5\\n[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y . He, Zero: Memory optimiza-\\ntions toward training trillion parameter models, in: SC20: International\\nConference for High Performance Computing, Networking, Storage and\\nAnalysis, IEEE, 2020, pp. 1–16. 2, 4, 23\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\\na unified view of parameter-e fficient transfer learning, arXiv preprint\\narXiv:2110.04366 (2021). 2, 20, 21\\n[39] Z. Hu, Y . Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\\nria, Llm-adapters: An adapter family for parameter-e fficient fine-tuning\\nof large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\\n20\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\\nefficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 21\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\\nFrom dense to sparse: Contrastive pruning for better pre-trained lan-\\n35guage model compression, in: Proceedings of the AAAI Conference on\\nArtificial Intelligence, V ol. 36, 2022, pp. 11547–11555. 2, 21\\n[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\\nAccurate and e fficient post-training quantization for large language\\nmodels, in: ICML, V ol. 202 of Proceedings of Machine Learning Re-\\nsearch, PMLR, 2023, pp. 38087–38099. 2, 20\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nCompression of generative pre-trained language models via quantiza-\\ntion, arXiv preprint arXiv:2203.10705 (2022). 2, 20\\n[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\\nGiraffe: Adventures in expanding context lengths in llms, arXiv preprint\\narXiv:2308.10882 (2023). 2, 17\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn: E fficient con-\\ntext window extension of large language models, arXiv preprint\\narXiv:2309.00071 (2023). 2, 17\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y .-H. Sung, Y . Yang,\\nLongt5: E fficient text-to-text transformer for long sequences, arXiv\\npreprint arXiv:2112.07916 (2021). 2, 17\\n[49] S. Chen, S. Wong, L. Chen, Y . Tian, Extending context window\\nof large language models via positional interpolation, arXiv preprint\\narXiv:2306.15595 (2023). 2, 17\\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\\nJ. Zhang, Z. Dong, et al., A survey of large language models, arXiv\\npreprint arXiv:2303.18223 (2023). 2, 3, 7\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\\nvey on word representation models: From classical to state-of-the-art\\nword representation language models, Transactions on Asian and Low-\\nResource Language Information Processing 20 (5) (2021) 1–35. 2, 3\\n[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\\ncessing via large pre-trained language models: A survey, arXiv preprint\\narXiv:2111.01243 (2021). 2, 3\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He, et al., A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\\n2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\\narXiv:2301.00234 (2022). 2, 7, 17\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 17\\n[56] Y . Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\\nQ. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y . Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (V ol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-\\ning of the Association for Computational Linguistics (V olume 1: Long\\nPapers), 2016, pp. 1715–1725. 4\\n[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\\nIEEE international conference on acoustics, speech and signal process-\\ning (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Ra ffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y . Lee, B. Sagot, et al., Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization in\\nnlp, arXiv preprint arXiv:2112.10508 (2021). 4\\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017). 4, 7\\n[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention withlinear biases enables input length extrapolation, in: International Con-\\nference on Learning Representations, 2022.\\nURL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\\n[66] J. Su, Y . Lu, S. Pan, A. Murtadha, B. Wen, Y . Liu, Roformer: En-\\nhanced transformer with rotary position embedding, arXiv preprint\\narXiv:2104.09864 (2021). 4, 9, 17\\n[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\\nwith sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\\n23\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\\nmemory-e fficient exact attention with io-awareness, Advances in Neural\\nInformation Processing Systems 35 (2022) 16344–16359. 4\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n[70] V . Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\\nmann machines, in: Proceedings of the 27th international conference on\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\npreprint arXiv:1606.08415 (2016). 4\\n[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overfitting, The\\njournal of machine learning research 15 (1) (2014) 1929–1958. 4\\n[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\\nKe, A. Goyal, Y . Bengio, A. Courville, C. Pal, Zoneout: Regular-\\nizing rnns by randomly preserving hidden activations, arXiv preprint\\narXiv:1606.01305 (2016). 4\\n[74] N. Shazeer, Glu variants improve transformer, arXiv preprint\\narXiv:2002.05202 (2020). 4\\n[75] Y . N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\\ngated convolutional networks, in: International conference on machine\\nlearning, PMLR, 2017, pp. 933–941. 4\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\narXiv:1607.06450 (2016). 4\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\nin Neural Information Processing Systems 32 (2019). 4\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\\ntransformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\\n[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\\nMegatron-lm: Training multi-billion parameter language models using\\nmodel parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\\n[81] \"bmtrain: E fficient training for big models.\".\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n[82] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\\ntac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\\nart natural language processing, in: Proceedings of the 2020 conference\\non empirical methods in natural language processing: system demon-\\nstrations, 2020, pp. 38–45. 5\\n[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\\nrin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\\nJax: composable transformations of python +numpy programs (2018).\\n5\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y . Liu, H. Huang, B. Wang, Y . You,\\nColossal-ai: A unified deep learning system for large-scale parallel train-\\ning, arXiv preprint arXiv:2110.14883 (2021). 5\\n[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe: A\\nfast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\\n(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\\nwork, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\\n162. 5\\n[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\\native style, high-performance deep learning library, Advances in neural\\ninformation processing systems 32 (2019). 5\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\\nscale machine learning., in: Osdi, V ol. 16, Savannah, GA, USA, 2016,\\npp. 265–283. 5\\n[89] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao,\\n36B. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and e fficient machine\\nlearning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\\nlion parameter models with simple and e fficient sparsity, The Journal of\\nMachine Learning Research 23 (1) (2022) 5232–5270. 5, 9\\n[91] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat, et al., Glam: E fficient scaling of language\\nmodels with mixture-of-experts, in: International Conference on Ma-\\nchine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 25\\n[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y . Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-P: Towards trillion\\nparameter language model with sparse heterogeneous computing, arXiv\\npreprint arXiv:2303.10845 (2023). 5, 10, 11, 23, 25\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, C. Ra ffel, What language model architecture and pretrain-\\ning objective works best for zero-shot generalization?, in: International\\nConference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou,\\nH.-W. Hon, Unified language model pre-training for natural language\\nunderstanding and generation, Advances in neural information process-\\ning systems 32 (2019). 6\\n[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\\nS. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\\nmodels, arXiv preprint arXiv:2001.08361 (2020). 6\\n[96] J. Ho ffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\\net al., Training compute-optimal large language models, arXiv preprint\\narXiv:2203.15556 (2022). 6, 9, 25, 29\\n[97] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\\nT. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\\nstruction meta learning through the lens of generalization, arXiv preprint\\narXiv:2212.12017 (2022). 7, 11, 17, 22, 25, 28\\n[98] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang, C. Gan,\\nPrinciple-driven self-alignment of language models from scratch with\\nminimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\\n7, 16\\n[99] A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\\nN. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\\nas a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\\n7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\\nP. Christiano, G. Irving, Fine-tuning language models from human pref-\\nerences, arXiv preprint arXiv:1909.08593 (2019). 7\\n[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\\ntion: Improving zero-shot and few-shot learning of language models via\\nchain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 11\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\\nining the power of symbolic tasks in instruction tuning, arXiv preprint\\narXiv:2304.07995 (2023). 7, 11\\n[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\\nD. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\\nlanguage models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 19, 22\\n[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022). 7, 19\\n[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Gri ffiths, Y . Cao, K. Narasimhan,\\nTree of thoughts: Deliberate problem solving with large language mod-\\nels, arXiv preprint arXiv:2305.10601 (2023). 7, 19\\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, S. Gelly, Parameter-e fficient transfer learn-\\ning for nlp, in: International Conference on Machine Learning, PMLR,\\n2019, pp. 2790–2799. 7, 20\\n[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\\nof large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\\n[108] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang, et al., Pangu- α: Large-scale autoregressive pre-\\ntrained chinese language models with auto-parallel computation, arXiv\\npreprint arXiv:2104.12369 (2021). 8, 22, 23, 25[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y . Cen, X. Zou, Z. Yang,\\nJ. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\\ntraining language models, AI Open 2 (2021) 65–68. 8, 30\\n[110] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation, arXiv preprint\\narXiv:2107.02137 (2021). 8, 25\\n[111] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, R. Salakhutdinov,\\nTransformer-xl: Attentive language models beyond a fixed-length con-\\ntext, arXiv preprint arXiv:1901.02860 (2019). 8\\n[112] O. Lieber, O. Sharir, B. Lenz, Y . Shoham, Jurassic-1: Technical details\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 23, 25\\n[113] Y . Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\\nficiencies of self-attention, Advances in Neural Information Processing\\nSystems 33 (2020) 22640–22651. 8, 11\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\\nmodels bring? intensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25\\n[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\\nL. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\\nshot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\\n23, 25\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Ho ffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\\nguage models: Methods, analysis & insights from training gopher, arXiv\\npreprint arXiv:2112.11446 (2021). 8, 9, 25, 28\\n[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti, et al.,\\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a\\nlarge-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 23, 25\\n[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\\nH. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\\nsource autoregressive language model, arXiv preprint arXiv:2204.06745\\n(2022). 9, 22, 23, 24, 25\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9\\n[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\\ncision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\\n[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\\nton, J. Dean, Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\\n[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\\natm 20b: Few-shot learning using a large-scale multilingual seq2seq\\nmodel, arXiv preprint arXiv:2208.01448 (2022). 9, 22, 23, 24, 25\\n[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\\narXiv preprint arXiv:2305.10403 (2023). 9, 25\\n[124] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Garcia,\\nH. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\\nwith 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\\n23, 25\\n[125] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\\nguage learning paradigms, in: The Eleventh International Conference\\non Learning Representations, 2022. 9, 10, 23, 24, 25\\n[126] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\\neral language model pretraining with autoregressive blank infilling, in:\\nProceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (V olume 1: Long Papers), 2022, pp. 320–335. 10\\n[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and e fficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023). 10, 22, 25\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\npreprint arXiv:2112.05682 (2021). 10\\n[129] V . A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\n37M. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\\ntransformer models, Proceedings of Machine Learning and Systems 5\\n(2023). 10\\n[130] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,\\nC. Xiong, Codegen: An open large language model for code with multi-\\nturn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 10,\\n22, 25, 28\\n[131] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\\nwards, Y . Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n10, 25, 29\\n[132] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\\ncode generation with alphacode, Science 378 (6624) (2022) 1092–1097.\\n10, 23, 25, 29\\n[133] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\narXiv preprint arXiv:1911.02150 (2019). 10\\n[134] R. Y . Pang, H. He, Text generation by learning from demonstrations,\\narXiv preprint arXiv:2009.07839 (2020). 10\\n[135] R. Dabre, A. Fujita, Softmax tempering for training neural machine\\ntranslation models, arXiv preprint arXiv:2009.09372 (2020). 10\\n[136] Y . Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and genera-\\ntion, arXiv preprint arXiv:2109.00859 (2021). 10\\n[137] R. Li, L. B. Allal, Y . Zi, N. Muennigho ff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\\nwith you!, arXiv preprint arXiv:2305.06161 (2023). 10, 25\\n[138] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\\nA. Poulton, V . Kerkez, R. Stojnic, Galactica: A large language model for\\nscience, arXiv preprint arXiv:2211.09085 (2022). 10, 23, 25, 29\\n[139] FairScale authors, Fairscale: A general purpose modular pytorch library\\nfor high performance and large scale training, https://github.com/\\nfacebookresearch/fairscale (2021). 10\\n[140] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y . Du, et al., Lamda: Language models\\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\\n[141] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 32\\n[142] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\\ncial chat model with hundreds of billions parameters, arXiv preprint\\narXiv:2305.12002 (2023). 11, 16, 25\\n[143] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\nformer language model with jax (2021). 12, 23\\n[144] N. Muennigho ff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\\nCrosslingual generalization through multitask finetuning, arXiv preprint\\narXiv:2211.01786 (2022). 11, 25, 28, 31\\n[145] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\\nDynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n[146] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue, et al., Llama-adapter v2: Parameter-e fficient visual in-\\nstruction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\\n[147] Openai. gpt-4 technical report (2023). 16, 34\\n[148] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nT. B. Hashimoto, Stanford alpaca: An instruction-following llama\\nmodel, https://github.com/tatsu-lab/stanford_alpaca\\n(2023). 16, 25, 28\\n[149] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng,\\nS. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).\\nURL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\\n28\\n[150] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n[151] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n[152] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprintarXiv:2304.06975 (2023). 16\\n[153] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\\nWizardlm: Empowering large language models to follow complex in-\\nstructions, arXiv preprint arXiv:2304.12244 (2023). 16\\n[154] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nD. Jiang, Wizardcoder: Empowering code large language models with\\nevol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\\n[155] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\\ning language models to support answers with verified quotes, arXiv\\npreprint arXiv:2203.11147 (2022). 16\\n[156] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders, et al., Webgpt: Browser-\\nassisted question-answering with human feedback, arXiv preprint\\narXiv:2112.09332 (2021). 16, 18, 19, 25, 31\\n[157] A. Glaese, N. McAleese, M. Tr˛ ebacz, J. Aslanides, V . Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\\nalignment of dialogue agents via targeted human judgements, arXiv\\npreprint arXiv:2209.14375 (2022). 16, 19, 25\\n[158] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\\nDirect preference optimization: Your language model is secretly a re-\\nward model, arXiv preprint arXiv:2305.18290 (2023). 16\\n[159] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\\nT. Zhang, Raft: Reward ranked finetuning for generative foundation\\nmodel alignment, arXiv preprint arXiv:2304.06767 (2023). 16\\n[160] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\\nresponses to align language models with human feedback without tears,\\narXiv preprint arXiv:2304.05302 (2023). 16\\n[161] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y . Li, H. Wang, Preference rank-\\ning optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 16\\n[162] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\\ntuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\\n16\\n[163] Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\\nA. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\\nlessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 16\\n[164] Y . Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang, T. B. Hashimoto, Alpacafarm: A simulation frame-\\nwork for methods that learn from human feedback, arXiv preprint\\narXiv:2305.14387 (2023). 16\\n[165] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\\nPrompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n16\\n[166] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši ¯ut˙e, A. Chen,\\nA. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\\nity for moral self-correction in large language models, arXiv preprint\\narXiv:2302.07459 (2023). 16\\n[167] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 16\\n[168] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y . Bai, S. Kadavath,\\nB. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\\nguage models to reduce harms: Methods, scaling behaviors, and lessons\\nlearned, arXiv preprint arXiv:2209.07858 (2022). 16, 28\\n[169] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\\nlish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 16\\n[170] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\\nN. McAleese, G. Irving, Red teaming language models with language\\nmodels, arXiv preprint arXiv:2202.03286 (2022). 16\\n[171] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\\ncontinual learners, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 6107–6122. 16\\n[172] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n[173] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\\nC. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17\\n[174] H. Chen, Y . Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y . Yanggong,\\nJ. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\\nof low training data instruction tuning, arXiv preprint arXiv:2305.09246\\n38(2023). 17\\n[175] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y . Mao, X. Ma, A. Efrat,\\nP. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\\narXiv:2305.11206 (2023). 17, 25, 28\\n[176] C. Han, Q. Wang, W. Xiong, Y . Chen, H. Ji, S. Wang, Lm-infinite: Sim-\\nple on-the-fly length generalization for large language models, arXiv\\npreprint arXiv:2308.16137 (2023). 17\\n[177] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y . Zemlyan-\\nskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y . Tay, et al., Colt5: Faster\\nlong-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 17\\n[178] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\\nLongnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\\narXiv:2307.02486 (2023). 17\\n[179] Y . Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: E ffi-\\ncient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 17\\n[180] N. Ratner, Y . Levine, Y . Belinkov, O. Ram, I. Magar, O. Abend,\\nE. Karpas, A. Shashua, K. Leyton-Brown, Y . Shoham, Parallel context\\nwindows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers), 2023, pp. 6383–6402. 17\\n[181] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 17\\n[182] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 17\\n[183] S. Borgeaud, A. Mensch, J. Ho ffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 17, 18, 33\\n[184] W. Zhong, L. Guo, Q. Gao, Y . Wang, Memorybank: Enhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 17\\n[185] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\\nReflexion: Language agents with verbal reinforcement learning, arXiv\\npreprint arXiv:2303.11366 14 (2023). 17, 19\\n[186] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\\ning llms with databases as their symbolic memory, arXiv preprint\\narXiv:2306.03901 (2023). 17\\n[187] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, G. Neubig, Active retrieval augmented generation, arXiv\\npreprint arXiv:2305.06983 (2023). 17, 18\\n[188] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, Y . Shoham, In-context retrieval-augmented language models,\\narXiv preprint arXiv:2302.00083 (2023). 17, 18, 33\\n[189] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 17\\n[190] D. Schuurmans, Memory augmented large language models are compu-\\ntationally universal, arXiv preprint arXiv:2301.04589 (2023). 17\\n[191] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\\ngeneral read-write memory for large language models, arXiv preprint\\narXiv:2305.14322 (2023). 17\\n[192] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\\nwork: Bm25 and beyond, Foundations and Trends ®in Information Re-\\ntrieval 3 (4) (2009) 333–389. 18\\n[193] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, D. Zhou,\\nRationale-augmented ensembles in language models, arXiv preprint\\narXiv:2207.00747 (2022). 18\\n[194] F. Zhang, B. Chen, Y . Zhang, J. Liu, D. Zan, Y . Mao, J.-G. Lou, W. Chen,\\nRepocoder: Repository-level code completion through iterative retrieval\\nand generation, arXiv preprint arXiv:2303.12570 (2023). 18\\n[195] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\\nO. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study, arXiv preprint\\narXiv:2304.06762 (2023). 18\\n[196] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 18[197] J. Liu, D. Shen, Y . Zhang, B. Dolan, L. Carin, W. Chen, What makes\\ngood in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\\n(2021). 18\\n[198] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 18\\n[199] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\\nmodels, arXiv preprint arXiv:2301.12652 (2023). 18\\n[200] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\narXiv preprint arXiv:2306.13421 (2023). 18\\n[201] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\\nlanguage model pre-training, in: International conference on machine\\nlearning, PMLR, 2020, pp. 3929–3938. 18\\n[202] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: E fficient and ef-\\nfective retrieval-augmented text generation, in: Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2023, pp. 1437–1447. 18\\n[203] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\nation, arXiv preprint arXiv:2107.07566 (2021). 18\\n[204] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\\naugmented language models through few-shot prompting for open-\\ndomain question answering, arXiv preprint arXiv:2203.05115 (2022).\\n18\\n[205] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\\ngpt: A general multi-modal assistant that can plan, execute, inspect, and\\nlearn, arXiv preprint arXiv:2306.08640 (2023). 18, 19\\n[206] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y . N. Wu, S.-C. Zhu,\\nJ. Gao, Chameleon: Plug-and-play compositional reasoning with large\\nlanguage models, arXiv preprint arXiv:2304.09842 (2023). 18, 19, 22\\n[207] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\\nRibeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\\nguage models, arXiv preprint arXiv:2303.09014 (2023). 18\\n[208] C.-Y . Hsieh, S.-A. Chen, C.-L. Li, Y . Fujii, A. Ratner, C.-Y . Lee, R. Kr-\\nishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\\nlarge language models, arXiv preprint arXiv:2308.00675 (2023). 18\\n[209] Y . Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y . Tian, S. Li, Restgpt:\\nConnecting large language models with real-world applications via rest-\\nful apis, arXiv preprint arXiv:2306.06624 (2023). 18\\n[210] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\\nguage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 18\\n[211] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 18\\n[212] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 18\\n[213] Y . Qin, S. Liang, Y . Ye, K. Zhu, L. Yan, Y . Lu, Y . Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000 +real-world apis, arXiv preprint arXiv:2307.16789 (2023). 18,\\n19\\n[214] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, Y . Zhuang, Hugginggpt: Solv-\\ning ai tasks with chatgpt and its friends in huggingface, arXiv preprint\\narXiv:2303.17580 (2023). 19, 33\\n[215] Y . Liang, C. Wu, T. Song, W. Wu, Y . Xia, Y . Liu, Y . Ou, S. Lu, L. Ji,\\nS. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\\ndation models with millions of apis, arXiv preprint arXiv:2303.16434\\n(2023). 19\\n[216] D. Surís, S. Menon, C. V ondrick, Vipergpt: Visual inference via python\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 19\\n[217] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\\nuser assistance systems, Business & Information Systems Engineering\\n58 (2016) 367–370. 19\\n[218] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n134 (1-2) (2002) 57–83. 19\\n[219] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\\nmulti-agent collaborative framework, arXiv preprint arXiv:2308.00352\\n(2023). 19\\n[220] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou, et al., The rise and potential of large language model\\n39based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 19\\n[221] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\\nX. Chen, Y . Lin, et al., A survey on large language model based au-\\ntonomous agents, arXiv preprint arXiv:2308.11432 (2023). 19\\n[222] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\\nshot planners: Extracting actionable knowledge for embodied agents,\\nin: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 19\\n[223] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\\ning with language model is planning with world model, arXiv preprint\\narXiv:2305.14992 (2023). 19, 33\\n[224] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y . Feng, L. Xue, R. Murthy,\\nZ. Chen, J. Zhang, D. Arpit, et al., Retroformer: Retrospective\\nlarge language agents with policy gradient optimization, arXiv preprint\\narXiv:2308.02151 (2023). 19, 33\\n[225] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\\nlogue: Embodied reasoning through planning with language models, in:\\n6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 19\\n[226] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\\nEmbodied finetuning for vision-language reasoning in robot manipula-\\ntion, arXiv preprint arXiv:2305.18898 (2023). 19, 20, 33\\n[227] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, A. Garg, Progprompt: Generating situated robot task plans\\nusing large language models, in: 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 19,\\n33\\n[228] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\\nChiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\\nfor robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 19\\n[229] X. Tang, A. Zou, Z. Zhang, Y . Zhao, X. Zhang, A. Cohan, M. Gerstein,\\nMedagents: Large language models as collaborators for zero-shot med-\\nical reasoning, arXiv preprint arXiv:2311.10537 (2023). 19\\n[230] A. Brohan, Y . Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\\nJ. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\\nGrounding language in robotic a ffordances, in: Conference on Robot\\nLearning, PMLR, 2023, pp. 287–318. 19, 33\\n[231] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\\nguided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\\n20\\n[232] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\\nnav: Grounding large language models for dynamic planning to navi-\\ngation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20\\n[233] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y . Su,\\nLlm-planner: Few-shot grounded planning for embodied agents with\\nlarge language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n[234] V . S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\\npreprint arXiv:2303.03480 (2023). 20\\n[235] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\\nrobot navigation, in: 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\\n[236] Y . Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\\nwith large language models for object rearrangement, arXiv preprint\\narXiv:2303.06247 (2023). 20, 33\\n[237] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, J. Tang, Gpt under-\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20\\n[238] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-e fficient tun-\\ning: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\\n20\\n[239] Y . Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\\nAdamix: Mixture-of-adapter for parameter-e fficient tuning of large lan-\\nguage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\\n[240] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nW. Chen, Lora: Low-rank adaptation of large language models, arXiv\\npreprint arXiv:2106.09685 (2021). 20, 21, 22\\n[241] X. Liu, K. Ji, Y . Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks, in: Pro-ceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (V olume 2: Short Papers), 2022, pp. 61–68. 20\\n[242] A. Razdaibiedina, Y . Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\\nProgressive prompts: Continual learning for language models, arXiv\\npreprint arXiv:2301.12314 (2023). 20\\n[243] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\\nwards adaptive prefix tuning for parameter-e fficient language model\\nfine-tuning, arXiv preprint arXiv:2305.15212 (2023). 20\\n[244] E. B. Zaken, S. Ravfogel, Y . Goldberg, Bitfit: Simple parameter-\\nefficient fine-tuning for transformer-based masked language-models,\\narXiv preprint arXiv:2106.10199 (2021). 20\\n[245] T. Dettmers, M. Lewis, Y . Belkada, L. Zettlemoyer, Llm. int8 ():\\n8-bit matrix multiplication for transformers at scale, arXiv preprint\\narXiv:2208.07339 (2022). 20, 21\\n[246] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq: Accurate\\npost-training quantization for generative pre-trained transformers, arXiv\\npreprint arXiv:2210.17323 (2022). 20\\n[247] X. Wei, Y . Zhang, Y . Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\\npression +: Accurate quantization of large language models by equiva-\\nlent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 20\\n[248] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\\naccurate post-training quantization and pruning, Advances in Neural In-\\nformation Processing Systems 35 (2022) 4475–4488. 20\\n[249] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, arXiv\\npreprint arXiv:2306.02272 (2023). 21\\n[250] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\\nW. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained language models, arXiv\\npreprint arXiv:2210.03858 (2022). 21\\n[251] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: E fficient\\nfinetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21\\n[252] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-\\nishnamoorthi, V . Chandra, Llm-qat: Data-free quantization aware train-\\ning for large language models, arXiv preprint arXiv:2305.17888 (2023).\\n21\\n[253] Y . Guo, A. Yao, H. Zhao, Y . Chen, Network sketching: Exploiting bi-\\nnary structure in deep cnns, in: Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n[254] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\\nMemory-e fficient fine-tuning of compressed large language models via\\nsub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\\n21\\n[255] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and e ffective pruning\\napproach for large language models, arXiv preprint arXiv:2306.11695\\n(2023). 21\\n[256] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 21\\n[257] L. Yin, Y . Wu, Z. Zhang, C.-Y . Hsieh, Y . Wang, Y . Jia, M. Pechenizkiy,\\nY . Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\\nmissing secret sauce for pruning llms to high sparsity, arXiv preprint\\narXiv:2310.05175 (2023). 21\\n[258] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nStructured pruning for e fficient generative pre-trained language models,\\nin: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 21\\n[259] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\\nguage model for few-shot learning, Advances in Neural Information Pro-\\ncessing Systems 35 (2022) 23716–23736. 21, 22\\n[260] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models,\\narXiv preprint arXiv:2301.12597 (2023). 21, 22\\n[261] H. Liu, C. Li, Q. Wu, Y . J. Lee, Visual instruction tuning, arXiv preprint\\narXiv:2304.08485 (2023). 21, 22\\n[262] K. Li, Y . He, Y . Wang, Y . Li, W. Wang, P. Luo, Y . Wang, L. Wang,\\nY . Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 21, 22\\n[263] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\\n40tailed video understanding via large vision and language models, arXiv\\npreprint arXiv:2306.05424 (2023). 21\\n[264] H. Zhang, X. Li, L. Bing, Video-llama: An instruction-tuned\\naudio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 21\\n[265] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\\nY . Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\\ndio captioning dataset for audio-language multimodal research, arXiv\\npreprint arXiv:2303.17395 (2023). 21\\n[266] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\\nllm: Multi-modal language modeling with image, audio, video, and text\\nintegration, arXiv preprint arXiv:2306.09093 (2023). 21\\n[267] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models,\\narXiv preprint arXiv:2304.10592 (2023). 22\\n[268] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020). 22\\n[269] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\\nS. Hoi, Instructblip: Towards general-purpose vision-language models\\nwith instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\\n[270] Z. Xu, Y . Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\\nshot learning via instruction tuning, arXiv preprint arXiv:2212.10773\\n(2022). 22\\n[271] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\\nChatbridge: Bridging modalities with large language model as a lan-\\nguage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\\n[272] L. Li, Y . Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y . Yang, J. Xu,\\nX. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\\nlingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\\n[273] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\\nH. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\\narXiv preprint arXiv:2305.14167 (2023). 22\\n[274] G. Luo, Y . Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\\nEfficient vision-language instruction tuning for large language models,\\narXiv preprint arXiv:2305.15023 (2023). 22\\n[275] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y . Qiao,\\nLlama-adapter: E fficient fine-tuning of language models with zero-init\\nattention, arXiv preprint arXiv:2303.16199 (2023). 22\\n[276] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\\nRobust speech recognition via large-scale weak supervision, in: Inter-\\nnational Conference on Machine Learning, PMLR, 2023, pp. 28492–\\n28518. 22\\n[277] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\\nmodal chain-of-thought reasoning in language models, arXiv preprint\\narXiv:2302.00923 (2023). 22\\n[278] J. Ge, H. Luo, S. Qian, Y . Gan, J. Fu, S. Zhan, Chain of thought prompt\\ntuning in vision language models, arXiv preprint arXiv:2304.07919\\n(2023). 22\\n[279] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\\ning, drawing and editing with visual foundation models, arXiv preprint\\narXiv:2303.04671 (2023). 22\\n[280] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\\nM. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\\nsoning and action, arXiv preprint arXiv:2303.11381 (2023). 22\\n[281] T. Wang, J. Zhang, J. Fei, Y . Ge, H. Zheng, Y . Tang, Z. Li, M. Gao,\\nS. Zhao, Y . Shan, et al., Caption anything: Interactive image descrip-\\ntion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\\n(2023). 22\\n[282] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\\nAdapting clip for powerful 3d open-world learning, arXiv preprint\\narXiv:2211.11682 (2022). 22\\n[283] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\\nsoning without training, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\\n22\\n[284] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\\nfusion with intra-and inter-modality attention flow for visual question\\nanswering, in: Proceedings of the IEEE /CVF conference on computer\\nvision and pattern recognition, 2019, pp. 6639–6648. 22[285] Z. Yu, J. Yu, Y . Cui, D. Tao, Q. Tian, Deep modular co-attention net-\\nworks for visual question answering, in: Proceedings of the IEEE /CVF\\nconference on computer vision and pattern recognition, 2019, pp. 6281–\\n6290. 22\\n[286] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\\nW. Chang, S.-F. Chang, Idealgpt: Iteratively decomposing vision\\nand language reasoning via large language models, arXiv preprint\\narXiv:2305.14985 (2023). 22\\n[287] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y . Qiao, P. Gao, H. Li,\\nPrompt, generate, then cache: Cascade of foundation models makes\\nstrong few-shot learners, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\\n22\\n[288] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\\nnormalization of self-attention, CoRR abs /1910.05895 (2019). 23\\n[289] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, V . Stoyanov, Roberta: A robustly optimized bert pre-\\ntraining approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\\n[290] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nD. Song, Koala: A dialogue model for academic research, Blog post\\n(April 2023).\\nURL https://bair.berkeley.edu/blog/2023/04/03/koala/\\n25\\n[291] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile: An\\n800gb dataset of diverse text for language modeling, arXiv preprint\\narXiv:2101.00027 (2020). 28, 30\\n[292] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. V on Werra, C. Mou, E. González Ponferrada, H. Nguyen,\\net al., The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset, Advances in Neural Information Processing Systems 35 (2022)\\n31809–31826. 28\\n[293] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n[294] Together Computer, Redpajama: An open source recipe to reproduce\\nllama training dataset (Apr. 2023).\\nURL https://github.com/togethercomputer/\\nRedPajama-Data 28\\n[295] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\\nTuning language models with (almost) no human labor, arXiv preprint\\narXiv:2212.09689 (2022). 28\\n[296] Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\\nD. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\\nharmless assistant with reinforcement learning from human feedback,\\narXiv preprint arXiv:2204.05862 (2022). 28\\n[297] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\\nJ. Steinhardt, Measuring massive multitask language understanding,\\narXiv preprint arXiv:2009.03300 (2020). 24, 29\\n[298] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\nthe imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models, arXiv preprint arXiv:2206.04615 (2022). 24, 29\\n[299] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\\nA multi-task benchmark and analysis platform for natural language un-\\nderstanding, arXiv preprint arXiv:1804.07461 (2018). 24, 29\\n[300] Y . Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\\neration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\\n29\\n[301] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y . Li, Y . Xu, K. Sun, D. Yu,\\nC. Yu, et al., Clue: A chinese language understanding evaluation bench-\\nmark, arXiv preprint arXiv:2004.05986 (2020). 29\\n[302] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\\nbenchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n[303] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y .-L. Boureau, Can\\nyou put it all together: Evaluating conversational agents’ ability to blend\\nskills, arXiv preprint arXiv:2004.08449 (2020). 29\\n[304] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY . Zhang, D. Narayanan, Y . Wu, A. Kumar, et al., Holistic evaluation of\\nlanguage models, arXiv preprint arXiv:2211.09110 (2022). 29\\n41[305] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\\nY . Song, T. Oh, et al., Klue: Korean language understanding evaluation,\\narXiv preprint arXiv:2105.09680 (2021). 29\\n[306] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\\nanswering challenge, Transactions of the Association for Computational\\nLinguistics 7 (2019) 249–266. 25, 29\\n[307] M. T. Pilehvar, J. Camacho-Collados, Wic: 10,000 example\\npairs for evaluating context-sensitive representations, arXiv preprint\\narXiv:1808.09121 6 (2018). 25, 29\\n[308] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\\nmodels, arXiv preprint arXiv:1609.07843 (2016). 25, 29\\n[309] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\\nsive transformers for long-range sequence modelling, arXiv preprint\\narXiv:1911.05507 (2019). 25, 29\\n[310] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\\nlarge-scale chinese question matching corpus, in: Proceedings of the\\n27th international conference on computational linguistics, 2018, pp.\\n1952–1962. 26, 29\\n[311] S. Iyer, N. Dandekar, K. Csernai, First quora dataset re-\\nlease: Question pairs, https://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs . 29\\n[312] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\\ncoreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\\n[313] M.-C. De Marne ffe, M. Simons, J. Tonhauser, The commitmentbank: In-\\nvestigating projection in naturally occurring discourse, in: proceedings\\nof Sinn und Bedeutung, V ol. 23, 2019, pp. 107–124. 29\\n[314] Z. Li, N. Ding, Z. Liu, H. Zheng, Y . Shen, Chinese relation extraction\\nwith multi-grained information and external linguistic knowledge, in:\\nProceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics, 2019, pp. 4377–4386. 29\\n[315] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\\nand relation extraction dataset for chinese literature text, arXiv preprint\\narXiv:1711.07010 (2017). 29\\n[316] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\\nlarge-scale domain-specific chinese corpus for sentence semantic equiv-\\nalence identification, in: Proceedings of the 2018 conference on empiri-\\ncal methods in natural language processing, 2018, pp. 4946–4951. 29\\n[317] B. Liu, D. Niu, H. Wei, J. Lin, Y . He, K. Lai, Y . Xu, Matching arti-\\ncle pairs with graphical decomposition and convolutions, arXiv preprint\\narXiv:1802.07459 (2018). 29\\n[318] P. Li, W. Li, Z. He, X. Wang, Y . Cao, J. Zhou, W. Xu, Dataset and neu-\\nral recurrent sequence labeling model for open-domain factoid question\\nanswering, arXiv preprint arXiv:1607.06275 (2016). 29\\n[319] N. Peng, M. Dredze, Named entity recognition for chinese social media\\nwith jointly trained embeddings, in: Proceedings of the 2015 conference\\non empirical methods in natural language processing, 2015, pp. 548–\\n554. 29\\n[320] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\\nnale generation: Learning to solve and explain algebraic word problems,\\narXiv preprint arXiv:1705.04146 (2017). 29\\n[321] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\\nlease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\\ntium (2011). 29\\n[322] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\\ncomplex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\\n[323] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\\nin social media: A case study of african-american english, arXiv preprint\\narXiv:1608.08868 (2016). 29\\n[324] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, J. Allen, A corpus and evaluation framework\\nfor deeper understanding of commonsense stories, arXiv preprint\\narXiv:1604.01696 (2016). 26, 29\\n[325] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\\nWord prediction requiring a broad discourse context, arXiv preprint\\narXiv:1606.06031 (2016). 26, 29\\n[326] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\\nrization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\\n[327] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\\nation with planning-based hierarchical variational model, arXiv preprintarXiv:1908.06605 (2019). 29\\n[328] J. Novikova, O. Dušek, V . Rieser, The e2e dataset: New challenges for\\nend-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\\n[329] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\\nfor cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\\n[330] Y . Bisk, R. Zellers, J. Gao, Y . Choi, et al., Piqa: Reasoning about phys-\\nical commonsense in natural language, in: Proceedings of the AAAI\\nconference on artificial intelligence, V ol. 34, 2020, pp. 7432–7439. 26,\\n29\\n[331] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\\ndistantly supervised challenge dataset for reading comprehension, arXiv\\npreprint arXiv:1705.03551 (2017). 26, 29, 31\\n[332] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nO. Tafjord, Think you have solved question answering? try arc, the ai2\\nreasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 26, 29,\\n31\\n[333] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost: Phys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29\\n[334] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering, arXiv\\npreprint arXiv:1809.02789 (2018). 29\\n[335] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\\nwebnlg +shared task overview and evaluation results (webnlg +2020),\\nin: Proceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG +), 2020. 29\\n[336] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\\nA chinese dataset for cant understanding with common sense and world\\nknowledge, arXiv preprint arXiv:2104.02704 (2021). 29\\n[337] G. Lai, Q. Xie, H. Liu, Y . Yang, E. Hovy, Race: Large-scale\\nreading comprehension dataset from examinations, arXiv preprint\\narXiv:1704.04683 (2017). 26, 29\\n[338] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang,\\nL. Zettlemoyer, Quac: Question answering in context, arXiv preprint\\narXiv:1808.07036 (2018). 27, 29\\n[339] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\\ntle use a laptop? a question answering benchmark with implicit reason-\\ning strategies, Transactions of the Association for Computational Lin-\\nguistics 9 (2021) 346–361. 29\\n[340] J. Boyd-Graber, B. Satino ff, H. He, H. Daumé III, Besting the quiz mas-\\nter: Crowdsourcing incremental classification games, in: Proceedings of\\nthe 2012 joint conference on empirical methods in natural language pro-\\ncessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29\\n[341] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\\nquestion answer matching using end-to-end character-level multi-scale\\ncnns, Applied Sciences 7 (8) (2017) 767. 29\\n[342] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\\nteraction networks for chinese medical question answer selection, IEEE\\nAccess 6 (2018) 74061–74071. 29\\n[343] C. Xu, J. Pei, H. Wu, Y . Liu, C. Li, Matinf: A jointly labeled large-scale\\ndataset for classification, question answering and summarization, arXiv\\npreprint arXiv:2004.12302 (2020). 29\\n[344] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y . Choi, Winogrande: An\\nadversarial winograd schema challenge at scale, Communications of the\\nACM 64 (9) (2021) 99–106. 25, 29\\n[345] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, Y . Choi, Hellaswag: Can a\\nmachine really finish your sentence?, arXiv preprint arXiv:1905.07830\\n(2019). 27, 29\\n[346] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\\nnatives: An evaluation of commonsense causal reasoning., in: AAAI\\nspring symposium: logical formalizations of commonsense reasoning,\\n2011, pp. 90–95. 29\\n[347] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\\nlenge, in: Thirteenth international conference on the principles of knowl-\\nedge representation and reasoning, 2012. 25, 27, 29\\n[348] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, arXiv preprint\\narXiv:1811.00937 (2018). 27, 29\\n[349] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y . Choi, Socialiqa:\\n42Commonsense reasoning about social interactions, arXiv preprint\\narXiv:1904.09728 (2019). 29\\n[350] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\\nlenging chinese machine reading comprehension, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 141–155. 29\\n[351] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\\ning the gap between human and machine commonsense reading compre-\\nhension, arXiv preprint arXiv:1810.12885 (2018). 29\\n[352] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000 +questions\\nfor machine comprehension of text, arXiv preprint arXiv:1606.05250\\n(2016). 28, 29\\n[353] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\\nK. Toutanova, Boolq: Exploring the surprising di fficulty of natural\\nyes/no questions, arXiv preprint arXiv:1905.10044 (2019). 28, 29\\n[354] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\\nable questions for squad, arXiv preprint arXiv:1806.03822 (2018). 28,\\n29\\n[355] D. Dua, Y . Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\\nA reading comprehension benchmark requiring discrete reasoning over\\nparagraphs, arXiv preprint arXiv:1903.00161 (2019). 28, 29\\n[356] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\\ntailment challenge, in: Machine learning challenges workshop, Springer,\\n2005, pp. 177–190. 28, 29\\n[357] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y . Bisk, Webqa: Mul-\\ntihop and multimodal qa, in: Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n28, 29\\n[358] Y . Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\\nevaluation on chinese machine reading comprehension, arXiv preprint\\narXiv:1709.08299 (2017). 29\\n[359] Y . Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\\nA span-extraction dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:1810.07366 (2018). 28, 29\\n[360] Y . Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\\nA sentence cloze dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:2004.03116 (2020). 29\\n[361] Y . Li, T. Liu, D. Li, Q. Li, J. Shi, Y . Wang, Character-based bilstm-crf\\nincorporating pos and dictionaries for chinese opinion target extraction,\\nin: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29\\n[362] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\\ning beyond the surface: A challenge set for reading comprehension\\nover multiple sentences, in: Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, V olume 1 (Long Papers), 2018,\\npp. 252–262. 29\\n[363] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\\nberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\\ntions: a benchmark for question answering research, Transactions of the\\nAssociation for Computational Linguistics 7 (2019) 453–466. 29\\n[364] C. C. Shao, T. Liu, Y . Lai, Y . Tseng, S. Tsai, Drcd: A chinese ma-\\nchine reading comprehension dataset, arXiv preprint arXiv:1806.00920\\n(2018). 29\\n[365] W. He, K. Liu, J. Liu, Y . Lyu, S. Zhao, X. Xiao, Y . Liu, Y . Wang, H. Wu,\\nQ. She, et al., Dureader: a chinese machine reading comprehension\\ndataset from real-world applications, arXiv preprint arXiv:1711.05073\\n(2017). 29\\n[366] H. Tang, J. Liu, H. Li, Y . Hong, H. Wu, H. Wang, Dureaderrobust: A\\nchinese dataset towards evaluating the robustness of machine reading\\ncomprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\\n[367] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\\nquestions, arXiv preprint arXiv:1707.06209 (2017). 29\\n[368] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\\nranking with kernel pooling, in: Proceedings of the 40th International\\nACM SIGIR conference on research and development in information\\nretrieval, 2017, pp. 55–64. 29\\n[369] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcli ffe, R. Morante,\\nQa4mre 2011-2013: Overview of question answering for machine read-\\ning evaluation, in: Information Access Evaluation. Multilinguality, Mul-\\ntimodality, and Visualization: 4th International Conference of the CLEF\\nInitiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-ceedings 4, Springer, 2013, pp. 303–320. 29\\n[370] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\\nreading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\\n[371] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y . Feng, X. Han,\\nZ. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\\nment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\\n[372] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\\ncompetence with apps, arXiv preprint arXiv:2105.09938 (2021). 28, 29\\n[373] Y . Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\\nin: Proceedings of the 2017 conference on empirical methods in natural\\nlanguage processing, 2017, pp. 845–854. 28, 29\\n[374] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\\nto solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\\n29\\n[375] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V . Le, C. Sutton, Program synthesis with\\nlarge language models, CoRR abs /2108.07732 (2021). 29\\n[376] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. V osoughi, H. W.\\nChung, Y . Tay, S. Ruder, D. Zhou, et al., Language models are mul-\\ntilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\\n(2022). 29\\n[377] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\\npreprint arXiv:1608.01413 (2016). 29\\n[378] S.-Y . Miao, C.-C. Liang, K.-Y . Su, A diverse corpus for evaluating\\nand developing english math word problem solvers, arXiv preprint\\narXiv:2106.15772 (2021). 29\\n[379] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\\nMawps: A math word problem repository, in: Proceedings of the 2016\\nconference of the north american chapter of the association for computa-\\ntional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29\\n[380] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\\nsimple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\\n29\\n[381] Y . Lai, C. Li, Y . Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\\nD. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\\ndata science code generation, in: International Conference on Machine\\nLearning, PMLR, 2023, pp. 18319–18345. 29\\n[382] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\\nlanguage models, arXiv preprint arXiv:2108.07732 (2021). 29\\n[383] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\\nsarial nli: A new benchmark for natural language understanding, arXiv\\npreprint arXiv:1910.14599 (2019). 29\\n[384] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\\ncorpus for sentence understanding through inference, arXiv preprint\\narXiv:1704.05426 (2017). 29\\n[385] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\\nnosing syntactic heuristics in natural language inference, arXiv preprint\\narXiv:1902.01007 (2019). 29\\n[386] J. Liu, L. Cui, H. Liu, D. Huang, Y . Wang, Y . Zhang, Logiqa: A chal-\\nlenge dataset for machine reading comprehension with logical reason-\\ning, arXiv preprint arXiv:2007.08124 (2020). 29\\n[387] P. Lewis, B. O ˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\\nuating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29\\n[388] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, V . Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\\nresentations, arXiv preprint arXiv:1809.05053 (2018). 29\\n[389] Y . Yang, Y . Zhang, C. Tar, J. Baldridge, Paws-x: A cross-\\nlingual adversarial dataset for paraphrase identification, arXiv preprint\\narXiv:1908.11828 (2019). 29\\n[390] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\\nsummary!, Topic-Aware Convolutional Neural Networks for Extreme\\nSummarization. ArXiv, abs (1808). 29\\n[391] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli ´c, A. Korhonen,\\nXcopa: A multilingual dataset for causal commonsense reasoning, arXiv\\npreprint arXiv:2005.00333 (2020). 27, 29\\n[392] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\\n43as a baseline for cross-lingual transfer in commonsense reasoning, arXiv\\npreprint arXiv:2106.12066 (2021). 29\\n[393] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V . Niko-\\nlaev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\\ntion answering in typologically diverse languages, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 454–470. 29\\n[394] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\\nMlsum: The multilingual summarization corpus, arXiv preprint\\narXiv:2004.14900 (2020). 29\\n[395] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\\nhuman falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29\\n[396] I. Augenstein, C. Lioma, D. Wang, L. C. Lima, C. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc: A real-world multi-domain\\ndataset for evidence-based fact checking of claims, arXiv preprint\\narXiv:1909.03242 (2019). 29\\n[397] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\\nlarge-scale dataset for fact extraction and verification, arXiv preprint\\narXiv:1803.05355 (2018). 29\\n[398] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\\nhate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\\n29, 31\\n[399] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\\nbias in pretrained language models, arXiv preprint arXiv:2004.09456\\n(2020). 29, 31\\n[400] A. Parrish, A. Chen, N. Nangia, V . Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\\nquestion answering, arXiv preprint arXiv:2110.08193 (2021). 29\\n[401] J. Zhao, T. Wang, M. Yatskar, V . Ordonez, K.-W. Chang, Gender bias\\nin coreference resolution: Evaluation and debiasing methods, arXiv\\npreprint arXiv:1804.06876 (2018). 29\\n[402] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\\nlenge dataset for measuring social biases in masked language models,\\narXiv preprint arXiv:2010.00133 (2020). 29\\n[403] S. Gehman, S. Gururangan, M. Sap, Y . Choi, N. A. Smith, Realtoxic-\\nityprompts: Evaluating neural toxic degeneration in language models,\\narXiv preprint arXiv:2009.11462 (2020). 29\\n[404] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\\nmetrics for measuring unintended bias with real data for text classifica-\\ntion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29\\n[405] O. Bojar, R. Chatterjee, C. Federmann, Y . Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V . Logacheva, C. Monz, et al., Find-\\nings of the 2016 conference on machine translation, in: Proceedings of\\nthe First Conference on Machine Translation: V olume 2, Shared Task\\nPapers, 2016, pp. 131–198. 29\\n[406] B. Loïc, B. Magdalena, B. Ond ˇrej, F. Christian, G. Yvette, G. Ro-\\nman, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\\n2020 conference on machine translation (wmt20), in: Proceedings of\\nthe Fifth Conference on Machine Translation, Association for Compu-\\ntational Linguistics„ 2020, pp. 1–55. 29\\n[407] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\\nmatching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\\n[408] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\\nwikipedia: Knowledge-powered conversational agents, arXiv preprint\\narXiv:1811.01241 (2018). 29\\n[409] H. Rashkin, E. M. Smith, M. Li, Y .-L. Boureau, Towards empathetic\\nopen-domain conversation models: A new benchmark and dataset, arXiv\\npreprint arXiv:1811.00207 (2018). 29\\n[410] E. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shuster, J. Urbanek,\\nD. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\\ntional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\\ntion: From Machine Learning to Intelligent Conversations, Springer,\\n2020, pp. 187–208. 29\\n[411] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\\nmulti-domain dialogue dataset towards multi-turn knowledge-driven\\nconversation, arXiv preprint arXiv:2004.04100 (2020). 29\\n[412] L. CO, Iflytek: a multiple categories chinese text classifier. competition\\nofficial website (2019). 29\\n[413] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\\npushshift reddit dataset, in: Proceedings of the international AAAI con-\\nference on web and social media, V ol. 14, 2020, pp. 830–839. 30[414] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\\nform question answering, arXiv preprint arXiv:1907.09190 (2019). 31\\n[415] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\\nBenchmarking generalization via in-context instructions on 1,600 +lan-\\nguage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\\n[416] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\\nM. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\\ntasking structured knowledge grounding with text-to-text language mod-\\nels, arXiv preprint arXiv:2201.05966 (2022). 31\\n[417] Q. Ye, B. Y . Lin, X. Ren, Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\\n(2021). 31\\n[418] V . Aribandi, Y . Tay, T. Schuster, J. Rao, H. S. Zheng, S. V . Mehta,\\nH. Zhuang, V . Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\\nmulti-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\\n(2021). 31\\n[419] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\\npus for sentence understanding through inference, in: Proceedings of\\nthe 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\nV olume 1 (Long Papers), Association for Computational Linguistics,\\nNew Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\\nN18-1101 .\\nURL https://aclanthology.org/N18-1101 29\\n[420] Y . Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\\nscrambling, in: Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, V olume 1 (Long and Short Papers), Associa-\\ntion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\\n1298–1308. doi:10.18653/v1/N19-1131 .\\nURL https://aclanthology.org/N19-1131 29\\n[421] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\\nGPT a general-purpose natural language processing task solver?, in: The\\n2023 Conference on Empirical Methods in Natural Language Process-\\ning, 2023.\\nURL https://openreview.net/forum?id=u03xn1COsO 31\\n[422] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\\nprehensive survey of its applications, challenges, limitations, and future\\nprospects, TechRxiv (2023). 31\\n[423] X. L. Dong, S. Moon, Y . E. Xu, K. Malik, Z. Yu, Towards next-\\ngeneration intelligent assistants leveraging llm techniques, in: Proceed-\\nings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, 2023, pp. 5792–5793. 31\\n[424] K. Pandya, M. Holia, Automating customer service using langchain:\\nBuilding custom open-source gpt chatbot for organizations, arXiv\\npreprint arXiv:2310.05421 (2023). 31\\n[425] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\\nR. Geng, et al., Can llm already serve as a database interface? a\\nbig bench for large-scale database grounded text-to-sqls, arXiv preprint\\narXiv:2305.03111 (2023). 31\\n[426] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\\nchatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\\n2023–02. 31\\n[427] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\\nsir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\\nlanguage models for decision support in personalized oncology, JAMA\\nNetwork Open 6 (11) (2023) e2343689–e2343689. 31\\n[428] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\\nmaroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\\nploring the potential of chat-gpt as a supportive tool for sialendoscopy\\nclinical decision making and patient information support, European\\nArchives of Oto-Rhino-Laryngology (2023) 1–6. 31\\n[429] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\\nData decentralisation of llm-based chatbot systems in chronic disease\\nself-management, in: Proceedings of the 2023 ACM Conference on In-\\nformation Technology for Social Good, 2023, pp. 205–212. 31\\n[430] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\\nhuman feedback for a therapy chatbot application (2023). 31\\n[431] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\\n44agents: A personalized llm-powered agent framework, arXiv preprint\\narXiv:2310.02374 (2023). 31\\n[432] K. V . Lemley, Does chatgpt help us understand the medical literature?,\\nJournal of the American Society of Nephrology (2023) 10–1681. 31\\n[433] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\\nnext-generation large language model (llm) or chatgpt is required for\\nbiomedical engineering and research, Annals of Biomedical Engineering\\n(2023) 1–4. 31\\n[434] Y . Du, S. Zhao, Y . Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\\ncalla dataset: Probing llms’ interactive knowledge acquisition from chi-\\nnese medical literature, arXiv preprint arXiv:2309.04198 (2023). 31\\n[435] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\\nS. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\\nlanguage models in medical education: Opportunities, challenges, and\\nfuture directions, JMIR Medical Education 9 (1) (2023) e48291. 31\\n[436] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\\nChatgpt passing usmle shines a spotlight on the flaws of medical educa-\\ntion (2023). 31\\n[437] S. Ahn, The impending impacts of large language models on medical\\neducation, Korean Journal of Medical Education 35 (1) (2023) 103. 31\\n[438] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\\n(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\\n(2023) 1–3. 31\\n[439] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\\nArtificial intelligence and public health: Evaluating chatgpt responses to\\nvaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 31\\n[440] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\\nTozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\\nai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 31\\n[441] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\\nformance of chatgpt and other large language models (llm) for scientific\\nand research advancements: a double-edged sword, International Re-\\nsearch Journal of Modernization in Engineering Technology and Science\\n5 (10) (2023) 875–899. 31, 32\\n[442] W. Dai, J. Lin, H. Jin, T. Li, Y .-S. Tsai, D. Gaševi ´c, G. Chen, Can large\\nlanguage models provide feedback to students? a case study on chatgpt,\\nin: 2023 IEEE International Conference on Advanced Learning Tech-\\nnologies (ICALT), IEEE, 2023, pp. 323–325. 32\\n[443] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\\nChatgpt for good? on opportunities and challenges of large language\\nmodels for education, Learning and individual di fferences 103 (2023)\\n102274. 32\\n[444] N. Rane, Enhancing the quality of teaching and learning through chat-\\ngpt and similar large language models: Challenges, future prospects,\\nand ethical considerations in education, Future Prospects, and Ethical\\nConsiderations in Education (September 15, 2023) (2023). 32\\n[445] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\\ngenerating chatbot’s dialogue for english as a foreign language learning,\\nInternational Journal of Advanced Computer Science and Applications\\n14 (6) (2023). 32\\n[446] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\\nthe impacts of chatgpt on future scientific work, SocArXiv (2023). 32\\n[447] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\\nscholarly writing: Is the integrity of the scientific discourse in jeopardy?,\\narXiv preprint arXiv:2311.06981 (2023). 32\\n[448] Y . Zheng, H. Y . Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\\nLarge language models for scientific synthesis, inference and explana-\\ntion, arXiv preprint arXiv:2310.07984 (2023). 32\\n[449] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\\nin scientific writing, PsyArXiv (2023). 32\\n[450] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\\nentific writing: a friend or a foe?, Reproductive BioMedicine Online\\n(2023). 32\\n[451] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\\nusing large language models, arXiv preprint arXiv:2303.05398 (2023).\\n32\\n[452] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\\non learning mathematical reasoning with large language models, arXiv\\npreprint arXiv:2308.01825 (2023). 32[453] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\\nR. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\\naugmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n32\\n[454] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\\nT. Lukasiewicz, Y . Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\\nlanguage models for mathematics through interactions, arXiv preprint\\narXiv:2306.01694 (2023). 32\\n[455] Y . Liu, T. Han, S. Ma, J. Zhang, Y . Yang, J. Tian, H. He, A. Li, M. He,\\nZ. Liu, et al., Summary of chatgpt-related research and perspective\\ntowards the future of large language models, Meta-Radiology (2023)\\n100017. 32\\n[456] J. Drápal, H. Westermann, J. Savelka, Using large language models\\nto support thematic analysis in empirical legal studies, arXiv preprint\\narXiv:2310.18729 (2023). 32\\n[457] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\\ning legal concepts with augmented large language models (gpt-4), arXiv\\npreprint arXiv:2306.09525 (2023). 32\\n[458] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\\nA. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\\nbench: A collaboratively built benchmark for measuring legal reasoning\\nin large language models, arXiv preprint arXiv:2308.11462 (2023). 32\\n[459] J. Cui, Z. Li, Y . Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases, arXiv\\npreprint arXiv:2306.16092 (2023). 32\\n[460] H. Yang, X.-Y . Liu, C. D. Wang, Fingpt: Open-source financial large\\nlanguage models, arXiv preprint arXiv:2306.06031 (2023). 32\\n[461] Y . Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\\nsurvey, in: Proceedings of the Fourth ACM International Conference on\\nAI in Finance, 2023, pp. 374–382. 33\\n[462] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model, arXiv preprint\\narXiv:2305.19352 (2023). 33\\n[463] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\\naction, in: ACM /IEEE International Conference on Human-Robot Inter-\\naction, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33\\n[464] Y . Ye, H. You, J. Du, Improved trust in human-robot collaboration with\\nchatgpt, IEEE Access (2023). 33\\n[465] Y . Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\\nknowledge from large language models for task and motion planning,\\nin: RSS 2023 Workshop on Learning for Task and Motion Planning,\\n2023. 33\\n[466] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\\nwith large language models, arXiv preprint arXiv:2305.05658 (2023).\\n33\\n[467] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\\nfor deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 33\\n[468] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\\ngers of stochastic parrots: Can language models be too big?, in: Pro-\\nceedings of the 2021 ACM conference on fairness, accountability, and\\ntransparency, 2021, pp. 610–623. 33\\n[469] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\\ndeep learning (still) requires rethinking generalization, Communications\\nof the ACM 64 (3) (2021) 107–115. 33\\n[470] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\\ntrained language models, arXiv preprint arXiv:2105.00828 (2021). 33\\n[471] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\\nNow (2019) 1–33. 33\\n[472] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\\nguage models still can’t plan (a benchmark for llms on planning and\\nreasoning about change), arXiv preprint arXiv:2206.10498 (2022). 33\\n[473] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chen, et al., Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models, arXiv preprint arXiv:2309.01219\\n(2023). 33\\n[474] A. Webson, E. Pavlick, Do prompt-based models really understand the\\nmeaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 33\\n[475] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\\nthought, let’s not think step by step! bias and toxicity in zero-shot rea-\\n45soning, arXiv preprint arXiv:2212.08061 (2022). 33\\n[476] X. Liu, H. Cheng, P. He, W. Chen, Y . Wang, H. Poon, J. Gao, Adversar-\\nial training for large neural language models, ArXiv (April 2020).\\nURL https://www.microsoft.com/en-us/research/\\npublication/adversarial-training-for-large-neural-language-models/\\n34\\n[477] E. Shayegani, M. A. A. Mamun, Y . Fu, P. Zaree, Y . Dong, N. Abu-\\nGhazaleh, Survey of vulnerabilities in large language models revealed\\nby adversarial attacks (2023). arXiv:2310.10844 . 34\\n[478] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\\nllm can fool itself: A prompt-based adversarial attack (2023). arXiv:\\n2310.13345 . 34\\n[479] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nM. Du, Explainability for large language models: A survey (2023).\\narXiv:2309.01029 . 34\\n[480] S. Huang, S. Mamidanna, S. Jangam, Y . Zhou, L. H. Gilpin, Can large\\nlanguage models explain themselves? a study of llm-generated self-\\nexplanations (2023). arXiv:2310.11207 . 34\\n[481] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\\nmean for a language model to preserve privacy?, in: Proceedings of the\\n2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 34\\n[482] R. Plant, V . Giu ffrida, D. Gkatzia, You are what you write: Pre-\\nserving privacy in the era of large language models, arXiv preprint\\narXiv:2204.09391 (2022). 34\\n[483] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\\nB. Ren, Y . Wang, Real-time execution of large-scale language models\\non mobile (2020). arXiv:2009.06823 . 34\\n[484] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y . Liu, M. Guo,\\nY . Zhu, Olive: Accelerating large language models via hardware-\\nfriendly outlier-victim pair quantization, in: Proceedings of the 50th\\nAnnual International Symposium on Computer Architecture, 2023, pp.\\n1–15. 34\\n[485] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\\nlanguage models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 34\\n[486] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\\nand policy implications for large language models: Guiding responsible\\ndevelopment and deployment, arXiv preprint arXiv:2308.02678 (2023).\\n34\\n[487] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\\nmodels: a three-layered approach, AI and Ethics (2023) 1–31. 34\\n46'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the connection to your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 17\n",
      "Python-dotenv could not parse statement starting at line 18\n",
      "Python-dotenv could not parse statement starting at line 19\n",
      "Python-dotenv could not parse statement starting at line 20\n",
      "Python-dotenv could not parse statement starting at line 21\n",
      "Python-dotenv could not parse statement starting at line 24\n",
      "Python-dotenv could not parse statement starting at line 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the GROQ And OpenAI API KEY \n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Uyama\\Downloads\\LANGCHAIN-PROJECTS\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## If you do not have open AI key use the below Huggingface embedding\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your LangChain vector store ... backed by Astra DB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store = Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name=\"qa_mini_demo\",\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\\nNick Barnesh, Ajmal Miani\\naUniversity of Engineering and Technology (UET), Lahore, Pakistan\\nbThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ncUniversity of Technology Sydney (UTS), Sydney, Australia\\ndCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\neKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\nfSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\ngThe University of Melbourne (UoM), Melbourne, Australia\\nhAustralian National University (ANU), Canberra, Australia',\n",
       " 'gThe University of Melbourne (UoM), Melbourne, Australia\\nhAustralian National University (ANU), Canberra, Australia\\niThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in',\n",
       " 'robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\\non a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\\nconcepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only',\n",
       " 'concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\\nprovide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\\nextensive informative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans, and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-',\n",
       " 'with machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi',\n",
       " '∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over years containing keywords \"Large\\nLanguage Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\\nguage Model +Alignment\".\\nPreprint submitted to Elsevier April 11, 2024arXiv:2307.06435v9  [cs.CL]  9 Apr 20242019T5(Oct)\\nGPT-3(May)\\n WebGPT (Dec)\\nOPT -IML\\nTK-Instruct (May)\\nmT0 (Dec)\\n Wizard -LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)',\n",
       " 'GPT-3(May)\\n WebGPT (Dec)\\nOPT -IML\\nTK-Instruct (May)\\nmT0 (Dec)\\n Wizard -LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard -Coder (Jun)\\nGoat\\nPanGu -α(Apr)\\nCPM -2(Jun)\\nGPT-NeoX -20B (Apr)\\nCodeGen (Mar)\\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2(Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020 2021 2022 2023 2024mT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic -1(Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0(Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN -U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\n PaLM2 (May)\\nGPT-4\\nPanGu -Σ(Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)',\n",
       " 'FLAN -U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\n PaLM2 (May)\\nGPT-4\\nPanGu -Σ(Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\\ntuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text',\n",
       " 'Language Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4], and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs. While\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).',\n",
       " 'a generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are',\n",
       " 'proposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot. Fine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,',\n",
       " 'ences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc. These abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widelyadopted in diverse settings including, multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas',\n",
       " 'tool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at a cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41]. Param-\\neter e fficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods',\n",
       " 'eter e fficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for e fficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions. Researchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained',\n",
       " 'contribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n•We present a survey on the developments in LLM research\\nproviding a concise comprehensive overview of the direc-\\ntion.\\n•We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n•We summarize major findings of the popular contributions',\n",
       " 'tion.\\n•We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n•We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners e ffec-\\ntively leverage this technology.\\n•In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\n2Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. E fficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nhensively, including background, pre-training, fine-tuning,\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.',\n",
       " '7. Challenges\\nhensively, including background, pre-training, fine-tuning,\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,architectures, training pipelines and strategies, fine-tuning, and\\nutilization in di fferent domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-',\n",
       " 'utilization in di fferent domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n32. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units',\n",
       " 'in details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other. Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-',\n",
       " 'independently of each other. Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input',\n",
       " 'favors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss di fferent attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).',\n",
       " 'weight values. We discuss di fferent attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs. To speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)',\n",
       " 'in calculating attention using GPUs. To speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU (x)=max(0,x) (1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product ( ⊗) of a linear\\ntransformation and a sigmoid transformed ( σ) linear projection\\nof the input given as:',\n",
       " 'network layer that is an element-wise product ( ⊗) of a linear\\ntransformation and a sigmoid transformed ( σ) linear projection\\nof the input given as:\\nGLU (x,W,V,b,c)=(xW+b)⊗σ(xV+c), (2)\\nwhere Xis the input of layer and l,W,b,Vandcare learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU (x,W,V,b,c)=max(0,xW+b)⊗,\\nGEGLU (x,W,V,b,c)=GELU (xW+b)⊗(xV+c),\\nS wiGLU (x,W,V,b,c,β)=S wishβ(xW+b)⊗(xV+c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with',\n",
       " 'ization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers',\n",
       " 'tation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross di fferent devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n42.7. Libraries\\nSome commonly used libraries for LLMs training are:',\n",
       " 'across devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n42.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can di fferenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.',\n",
       " 'ate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write e fficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]: Provides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]: A deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.',\n",
       " 'a pythonic coding style.\\nTensorflow [88]: A deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C ++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based. Classifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of',\n",
       " 'essential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based. Classifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can a ffect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based',\n",
       " 'documents, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The di fference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.',\n",
       " 'the attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output. Here, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the',\n",
       " 'where the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an e fficient\\nsparse architecture that o ffers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives',\n",
       " 'models and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives. For\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting /utilization. Prompting LLMs to generate responses is possible at',\n",
       " 'Figure 5.\\n5Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting /utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language',\n",
       " 'future context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model',\n",
       " 'dataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of di fferent training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for di fferent\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-',\n",
       " 'occasionally the literature uses the term alignment for di fferent\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with di fferent\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are di fferent styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.',\n",
       " 'various tasks [6, 15]. However, to improve the performance for\\n6a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries e ffectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,',\n",
       " 'on formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-',\n",
       " '“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-',\n",
       " 'is used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting /Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing di fferent prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-',\n",
       " 'guide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task',\n",
       " 'context learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning. Generating\\nreasons is possible only by using di fferent prompting styles,whereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency: Improves CoT performance by generat-',\n",
       " 'step-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency: Improves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next',\n",
       " 'Multi-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We',\n",
       " 'of research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as',\n",
       " 'eling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language',\n",
       " 'decide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu- α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The',\n",
       " 'ulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α[108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na=pnWq\\nhWk\\nhT HT\\nL (3)\\nCPM-2 [12]: Cost-e fficient Pre-trained language Models']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50 headlines.\n"
     ]
    }
   ],
   "source": [
    "astra_vector_store.add_texts(texts[:50])\n",
    "\n",
    "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the QA cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION: \"what is tokenization\"\n",
      "ANSWER: \"According to the provided context, tokenization is an essential pre-processing step in LLM (Large Language Model) training that parses the text into non-decomposing units called tokens. Tokens can be characters, subwords, symbols, or words, depending on the tokenization process. Some common tokenization schemes used in LLMs include wordpiece, byte pair encoding (BPE), and unigramLM.\"\n",
      "\n",
      "FIRST DOCUMENTS BY RELEVANCE:\n",
      "    [0.7522] \"in details to the original works.\n",
      "2.1. Tokenization\n",
      "Tokenization [59] is an essentia ...\"\n",
      "    [0.7440] \"utilization in di fferent domains. Section 4 highlights the config-\n",
      "uration and para ...\"\n",
      "    [0.6600] \"favors using recent tokens for attention.\n",
      "RoPE [66]: It rotates query and key repres ...\"\n",
      "    [0.6550] \"independently of each other. Moreover, the attention mod-\n",
      "ule in the transformer doe ...\"\n"
     ]
    }
   ],
   "source": [
    "first_question = True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
    "\n",
    "    if query_text.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if query_text == \"\":\n",
    "        continue\n",
    "\n",
    "    first_question = False\n",
    "\n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
    "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
